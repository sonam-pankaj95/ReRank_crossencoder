{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 41584\n",
       "})"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data['chunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "docker: Error response from daemon: Conflict. The container name \"/rag-openai-qdrant\" is already in use by container \"6171c0f43b856ad8715e93292256bdafa386cacc7e6645936b72581f986479b3\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "See 'docker run --help'.\n"
     ]
    }
   ],
   "source": [
    "!docker run -p \"6333:6333\" -p \"6334:6334\" --name \"rag-openai-qdrant\" --rm -d qdrant/qdrant:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='knowledge-base')])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import qdrant_client\n",
    "\n",
    "qdrantclient = qdrant_client.QdrantClient(\"http://localhost:6333\", prefer_grpc=True)\n",
    "qdrantclient.get_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==0.28 in d:\\projects\\new\\env\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in d:\\projects\\new\\env\\lib\\site-packages (from openai==0.28) (2.31.0)\n",
      "Requirement already satisfied: tqdm in d:\\projects\\new\\env\\lib\\site-packages (from openai==0.28) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in d:\\projects\\new\\env\\lib\\site-packages (from openai==0.28) (3.8.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\projects\\new\\env\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\new\\env\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\projects\\new\\env\\lib\\site-packages (from requests>=2.20->openai==0.28) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\new\\env\\lib\\site-packages (from requests>=2.20->openai==0.28) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\projects\\new\\env\\lib\\site-packages (from aiohttp->openai==0.28) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\projects\\new\\env\\lib\\site-packages (from aiohttp->openai==0.28) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\projects\\new\\env\\lib\\site-packages (from aiohttp->openai==0.28) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\projects\\new\\env\\lib\\site-packages (from aiohttp->openai==0.28) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\projects\\new\\env\\lib\\site-packages (from aiohttp->openai==0.28) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\projects\\new\\env\\lib\\site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
      "Requirement already satisfied: colorama in d:\\projects\\new\\env\\lib\\site-packages (from tqdm->openai==0.28) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "from num2words import num2words\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['89ba5e7298484e85bc779523258013d8',\n",
       " 'd644d1fdd8fe471f9372bde0fe2c69f7',\n",
       " 'd4f0f86ba0564cb48b02a66c16c52d0d',\n",
       " '2e8a8e27da7b432b802e97ef12df0173',\n",
       " '9be15d99d0a842e69de3b87d9aba5228',\n",
       " '3ca8c3ef888b4f4099b06f5cce39f9d5',\n",
       " '3471add2092742eeae87fb1a3f96c5f3',\n",
       " '8794e1f5b06c43e9980d0a62ce5caf11',\n",
       " '2311a731980542b1bcb2de6f2a6f7e5f',\n",
       " '80886e7a79cd4080a085d2a9d600af49',\n",
       " '8f79cf52956d46298948f8b2384759c2',\n",
       " '4155bd52321c46eebaacd9ce4a65da69',\n",
       " 'ecd2706b07ac443ab03e0929311c7b08',\n",
       " 'b6ee16fc4a4743488de40cff4ec72d53',\n",
       " '8da518bee2894d278b8dd90ee969b3f5',\n",
       " '8ac9258f58934021b7974cd82a8b183a',\n",
       " '8b941e956c4f40f19fac95d60eef47f3',\n",
       " '2a250fe9b80d4773b2af7d59b7c39269',\n",
       " '53f9d0dc44f24011bd1a8ebd98dd421a',\n",
       " 'a5ddf2082997410184f2b46485e0ef70',\n",
       " 'ed33ef7e8eee4037bbd3d5d914a63591',\n",
       " '01bf2dbfece24d5284452524f12c4f62',\n",
       " '00f6809cf4df4c6cad5e24b621760329',\n",
       " '3a4a18f0bcad4666ac36aec33f70fa06',\n",
       " 'bb958413261046009d33388709e301ac',\n",
       " '512ee1dcf5a146cf8a3d0241ce9205b6',\n",
       " '4fd7e80458d4406f9aaf5faa210a361d',\n",
       " 'd33cf88deb354c4695aba3f5c2440cc2',\n",
       " 'd351a68ed95d4b4985d3269fd3ba6503',\n",
       " '80e67a755146446eade7fc1b7451fe43',\n",
       " '3538f1632a2240e09f33d7cbc4c94e73',\n",
       " 'bed2d1dd25ef4b5eaac32018f93cb212',\n",
       " '50b397c716b744e4a93736fe8c0ba72d',\n",
       " '908ceef57f784e1c927cee65ced8bb40',\n",
       " '5eb6c9d7cfa84aa9a72a56be99b2c34d',\n",
       " 'b80b1a69c3974afc9d5d7a930c0049d9',\n",
       " 'd2aa678e115942dda2fc4723691f3c66',\n",
       " 'da09efbeed584d049919a078e4d53f14',\n",
       " 'e8ca6c2535104a789cbd5c73af700525',\n",
       " '9a286f7a1b8a4bacb76ebe911d0c10b7',\n",
       " '88cc29166abd4ea6b8a88a3493b53b8f',\n",
       " '678997e91bde475899e2eeae297c5cdb',\n",
       " 'e9680a5e584a4030b774f15d9359be03',\n",
       " '74209eb1aa6f4fd6accd6cf13d4c000c',\n",
       " '5b95144d758f4cda9dba6a5b4c0fa8d0',\n",
       " '4f9421e80c0646b4afe42a2431308715',\n",
       " '6fedbc11d441496bb7313170a2f7084c',\n",
       " '2817e09c388748819e389bdae1c6f4c5',\n",
       " '2c9c551f059743db88a97190e26df89b',\n",
       " 'a951e2e061da466bbd215dd67180ca12',\n",
       " 'e02e03200221499694f88725022c822b',\n",
       " '62253f63156b4f72889e3e0582edf222',\n",
       " 'e4dd8635011c4baca80cd73203ceca5d',\n",
       " '42f7a321dd1a4e80bf8416ca4871669d',\n",
       " '96a04a892cd74558816a143bbf907631',\n",
       " '68bf2356c2d142909b032cee6561c25b',\n",
       " '92fe4705bd7140d890727d4716578219',\n",
       " '3c96524d9c16419eb54b7ce594fa1f2c',\n",
       " 'e114f411495946e69690b89118a02312',\n",
       " 'a374007bdffe458b8968054fb85e7e74',\n",
       " '75646f9563f24c5a923da2d64e923028',\n",
       " '4b8f692d971648c498d903ca565469a4',\n",
       " 'ec0cda7ef6ba47eb984e4323f475f5ef',\n",
       " '4ce2648d64b14b3fbffb7649aee4ecc8',\n",
       " 'b5874512833145cc81434f8147392059',\n",
       " '6323a7107ba1494f973819a319b689ef',\n",
       " '0c052427b8334178a6125dfe69442197',\n",
       " 'a30c7d8f32904461929b4000e09a24d2',\n",
       " 'e3fbf83ebbb4450099f10846a98cdeec',\n",
       " 'c02daf0009f04adbb9d20009abd9d4ff',\n",
       " '9d8f8cfabcd7408dbb9d64dd2dad00db',\n",
       " '548185bf1ede4729b4d79de4b79c346e',\n",
       " 'e2587b38a7994c8192f0c3dbc7946251',\n",
       " '7f335f5637544caeb9621b95346a2231',\n",
       " 'cd04b1a60ee54ce0ae5fe3a97c6cd6e5',\n",
       " 'c2b9b03796ed42d2a5e7822adfef29b4',\n",
       " 'be5dcc716dec4e579f19d90533529ec8',\n",
       " 'f74ce3665e3d4f33a7984a83b1a3634a',\n",
       " 'fc642a82e2264b908618690fab45e4f3',\n",
       " '8e2feca5dbc0437ebb55f3dc69d6882b',\n",
       " '714db71f39fb48b990d3dd269b4e64ba',\n",
       " 'd8be17c8acd34fa99b4cf969effe9087',\n",
       " '4b39d583b94c4322ac1de6789a7804ce',\n",
       " '33e9ff48305449aa9a0e55e54c57c9fe',\n",
       " '17caa62e42934cc4ab34a5c1c56be688',\n",
       " '87c26c9b5ffb4a6d829fa07983835c0e',\n",
       " '42b2233c322140fdb2b84fbfd7b28137',\n",
       " '027ad4e924eb4ab7a6e76075bd251f84',\n",
       " '618956bcf361416a83a06f17191ce098',\n",
       " '97ba8f35be6f474283ead2bb5e9d85a7',\n",
       " '5ecad538aa64402ca56afb89f3ca69a1',\n",
       " '19923738095f4b5da0d1f40bca2efe1e',\n",
       " '083f6b591df047968a8f6f640c3bf51d',\n",
       " 'cd638d4fc64347d4b308dc5689943106',\n",
       " 'e56531528e1c49ae8ee1c40f444812a4',\n",
       " 'ec376cc638f4475e90c523dcd10a827e',\n",
       " '9161b3fd31a74b868fc7afbf1eddc4b9',\n",
       " 'cb9da21085174a40897f2358c13217e6',\n",
       " '363893c2570a4abeb871608597e9ff3a',\n",
       " '722d0bcd04534c2bb4e2330e6f04f3cd',\n",
       " 'b2edf2189d9d4d73b11197b22dd06a3a',\n",
       " '8a895ef6a6434e9d86432370a9f0d673',\n",
       " 'b0b6899a1b43433d9b4cd146f19b164a',\n",
       " 'fddd1b46fcba446589d4ffe01f29bb47',\n",
       " '08adc0b27ade43209ff4d2655c796734',\n",
       " 'e6c02d30dbb14e9e82db7c128cf7d6e8',\n",
       " '215cc8d2cf3b450bab5101e78be0246d',\n",
       " '253e474b30814b5fad15ef09b5b9235a',\n",
       " '4b26be3224a14096af4e249ab8409d3a',\n",
       " 'ba8d2fa6320e4f60ba0eed4dd46fa8b1',\n",
       " 'ce225f3e08f54e6f92b73ec2528b39ed',\n",
       " 'ad1d2433418b4ea1ac1ad652c2320e57',\n",
       " 'ecba92d266f7405b86a3b6d055072cbb',\n",
       " '7be922e3204f45b080dde27d1bd6ff83',\n",
       " 'b7d4c0b1d9414d01a6c47825057ddf89',\n",
       " '868a9eb82690412ba4b26aa6b6b45292',\n",
       " 'ada1369db3434c6a991f856d1957827b',\n",
       " '3f5d77a1ffc6402bbd7149991ca78d03',\n",
       " '8f291a814fc64de699c845dc03998c79',\n",
       " 'a80455900db2408d840682e7a38b3bb6',\n",
       " '14f63b0d45cc42a795b9b02ec27899e5',\n",
       " '2aff2d8ba7b4430e9c28f314b315714f',\n",
       " 'f52ddca6fa8048bd96858898465e5522',\n",
       " '54f62ba78272474196cfc47fa9475082',\n",
       " '78358a5922954944a558906650aa36d7',\n",
       " 'afb8306a899947c49041c361967f1b45',\n",
       " '0bfd735ab34e43529ac319eafe5faf24',\n",
       " '3d9fef42cbef414798dea45a376348a7',\n",
       " 'fe76f90a5ef7450d8a86d3fc527e4219',\n",
       " 'fdf73a57ea644ac4bc1b794fe25c2e70',\n",
       " '4ebc166c6bdd4aa99671bbc668760e17',\n",
       " '086bf934fa6e440597dcbd84afb04cca',\n",
       " '68ddd80c6b2f4c909949521169efb8b4',\n",
       " '977bc1a98dde4d8fab055c7eae7cdc22',\n",
       " 'a150b138f92f4833bba028a675a15d8d',\n",
       " '8d49717cccde4363af795b939a182ddb',\n",
       " 'd21c3ec5f74e48c6bc6dd45a241fc9a1',\n",
       " '804df0376bdd4364a12564fe2037b18e',\n",
       " '2220ed4e141f4d9a901b51472ed3adf5',\n",
       " '544801b9fdaa451ab323bd39ffe1a499',\n",
       " '52006069e674436eb2611018f77efd73',\n",
       " 'a05f735197824571a689332cef54f69f',\n",
       " '5e261c3db5b648dfb4e8c0866e4410e2',\n",
       " '14ed5bb2eb524149814a2a9629854a48',\n",
       " 'ac2259deb25248848576e3b7047013b0',\n",
       " '0ff0dfda2ac24cfd9dfaba4759227adc',\n",
       " '87cd918250a648e8a2a40801840d3453',\n",
       " '1d2b7e7c157448e6a9dcd6212d74e0fc',\n",
       " '46cae9806641487dab6edcfe277c82c6',\n",
       " '84cac5e6f5004e058d5ad7b61f5cafc2',\n",
       " 'a1a1f2b73c8446a980715b6d77ad217e',\n",
       " 'f6f596fd0fc44bceb9db6ccc559d0ebc',\n",
       " 'c25a51f8107a446dbc4413cf14c79b54',\n",
       " 'a2f5454c137c483096a00988a3f40af3',\n",
       " '9d15e736d4e346f79a9a09caaefdc62e',\n",
       " '238d0e0676844d53a2420b38928cc4bf',\n",
       " '96b6b913ea03456687aa089d488c1d5b',\n",
       " 'c71668107e824953bd6f9e22e4635369',\n",
       " '72de3eac36014fc69b1ca923961c7beb',\n",
       " '0108689699274c26a43ada87debcddcc',\n",
       " 'ccaf528a44334e03a240e30eb2969f0f',\n",
       " '8a0363a54005412fb4f650f26045c9f3',\n",
       " '2766271b6ffa45eba250470ffb2d730d',\n",
       " 'f6d8d72605f24de7abb9dcb663f6aa5d',\n",
       " '573775394c0d4a489a5af3e4439a43a9',\n",
       " '1b451c8caf2240fdb09d87a40c45b84d',\n",
       " 'ae2a6c8d6d96441bb6363e37488be075',\n",
       " '1147a40f94df4b4e9af6cee2afa230cd',\n",
       " '53e4672f073744d5b6134d51e16270f9',\n",
       " '90a4ad18ab3a4420b0eb6d1b659f1e8a',\n",
       " '0c915ce8f94140db890051fca0fbd5c3',\n",
       " '74051634d1c14346a691d76052104d74',\n",
       " '95232905a49b41bc83122660f93416e0',\n",
       " 'e97d062b50464756887dd6c4386e8319',\n",
       " '69e00829b2744202bfd6f7cade55e597',\n",
       " '2ff2cbde3542494eaf655545d0a9c66e',\n",
       " '2c63c516b3f348158e25a5cdb18293aa',\n",
       " 'e4925c05601549e6b04e7a6cc5745fe2',\n",
       " '2ec70ab58cce480e834bbe5248ac665f',\n",
       " '0f35563bec664a7c85cba149a9e3bab8',\n",
       " '5150cc07c9554b9c897c598c6ea6b96c',\n",
       " '61c49ebc0b2f4eb0b68f9e72a72b63cf',\n",
       " '80481cdd6a1042b6ac016d84725224d6',\n",
       " '9d0f9a54dbe14fd79ff9c0a6aafe3c13',\n",
       " 'd44a759a766e4666ba750d5374c249eb',\n",
       " '9cc07f4e76504b4fa80fdeb03447be8f',\n",
       " 'e3b7578a9e0c4e6ba4fd12fe5f017cf0',\n",
       " '630da2a4cff74f2f9039519519d31638',\n",
       " '05c06b15526e4aa991b90beeec4d85d3',\n",
       " '0ff37b107a454045bac881ee0406e085',\n",
       " '1b93016a989d4dcca12267e5a9eb1394',\n",
       " '77a19600ad6d4a8cbfaac8d5520bb767',\n",
       " '808f9cbd4a3c479ca026f6c31e42894b',\n",
       " '711cd186ed504ab8882614a05539f381',\n",
       " '96793c7d20814b83bd8fd512b813cea9',\n",
       " '12fe8f15bd8d4c449b0a3d7879971ce1',\n",
       " '44ca46a063fc4b06ab9357f81533ed78',\n",
       " '831c032dfeea4092a1c4339720c674b4',\n",
       " '8fff20ba739f45a59b9428d1b74f8832',\n",
       " '800c6c8ca8ae4bb4afedf5844e84d3fd']"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrantclient.add(\n",
    "    collection_name=\"knowledge_base\",\n",
    "    documents=data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what is language model?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(question: str, n_points: int = 1) -> str:\n",
    "    results = qdrantclient.query(\n",
    "        collection_name=\"knowledge_base\",\n",
    "        query_text=question,\n",
    "        limit=n_points,\n",
    "    )\n",
    "\n",
    "    context = \"\\n\".join(r.document for r in results)\n",
    "\n",
    "    metaprompt = f\"\"\"\n",
    "    You are a software architect. \n",
    "    Answer the following question using the provided context. \n",
    "    \n",
    "    \n",
    "    Question: {question.strip()}\n",
    "    \n",
    "    Context: \n",
    "    {context.strip()}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": metaprompt}\n",
    "    ]\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='In the given context, there is no direct explanation or definition of a language model. It appears to be a statistical breakdown or analysis of different query types in a training set. Without further information, it is not possible to provide a specific definition of a language model based on this context.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='In the given context, there is no direct explanation or definition of a language model. It appears to be a statistical breakdown or analysis of different query types in a training set. Without further information, it is not possible to provide a specific definition of a language model based on this context.', role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(Query):\n",
    "    return chat(Query)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\"text\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\new\\env\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "# Load a pre-trained cross-encoder model\n",
    "cross_encoder_model = CrossEncoder('cross-encoder/nli-deberta-v3-large')\n",
    "\n",
    "# Query sentence\n",
    "query_sentence = \"What is language model?\"\n",
    "\n",
    "# List of sentences to compare against\n",
    "sentence2_list = data[:100]\n",
    "\n",
    "# Encode the sentences\n",
    "sentence_pairs = [(query_sentence, sentence2) for sentence2 in sentence2_list]\n",
    "similarity_scores = cross_encoder_model.predict(sentence_pairs)\n",
    "\n",
    "df_data = {'Query': [query_sentence] * len(sentence2_list),\n",
    "           'Sentence2': sentence2_list,\n",
    "           'Similarity Score': similarity_scores}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_data = {'Query': [],\n",
    "           'Sentence2': [],\n",
    "           'Similarity Score': []}\n",
    "\n",
    "# Populate the DataFrame\n",
    "for sentence2, score in zip(sentence2_list, similarity_scores):\n",
    "    df_data['Query'].append(query_sentence)\n",
    "    df_data['Sentence2'].append(sentence2)\n",
    "    df_data['Similarity Score'].append(-(score[1]))\n",
    "\n",
    "df = pd.DataFrame(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Query': ['What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?',\n",
       "  'What is language model?'],\n",
       " 'Sentence2': ['DistilBERT, a distilled version of BERT: smaller,\\nfaster, cheaper and lighter\\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\\nHugging Face\\n{victor,lysandre,julien,thomas}@huggingface.co\\nAbstract\\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\\nin Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains\\nchallenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be ﬁnetuned with good performances on a wide range of tasks like its larger counterparts.\\nWhile most prior work investigated the use of distillation for building task-speciﬁc\\nmodels, we leverage knowledge distillation during the pre-training phase and show\\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\\nof its language understanding capabilities and being 60% faster. To leverage the\\ninductive biases learned by larger models during pre-training, we introduce a triple\\nloss combining language modeling, distillation and cosine-distance losses. Our\\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its',\n",
       "  'loss combining language modeling, distillation and cosine-distance losses. Our\\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its\\ncapabilities for on-device computations in a proof-of-concept experiment and a\\ncomparative on-device study.\\n1 Introduction\\nFigure 1: Parameter counts of several recently released\\npretrained language models.The last two years have seen the rise\\nof Transfer Learning approaches in\\nNatural Language Processing (NLP)\\nwith large-scale pre-trained language\\nmodels becoming a basic tool in\\nmany NLP tasks [Devlin et al., 2018,\\nRadford et al., 2019, Liu et al., 2019].\\nWhile these models lead to signiﬁcant improvement, they often have\\nseveral hundred million parameters\\nand current research1on pre-trained\\nmodels indicates that training even\\nlarger models still leads to better performances on downstream tasks.\\nThe trend toward bigger models\\nraises several concerns. First is the\\nenvironmental cost of exponentially scaling these models’ computational requirements as mentioned\\nin Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device\\nin real-time has the potential to enable novel and interesting language processing applications, the\\ngrowing computational and memory requirements of these models may hamper wide adoption.',\n",
       "  'in real-time has the potential to enable novel and interesting language processing applications, the\\ngrowing computational and memory requirements of these models may hamper wide adoption.\\n1See for instance the recently released MegatronLM ( https://nv-adlr.github.io/MegatronLM )\\nEMC^2: 5th Edition Co-located with NeurIPS’19arXiv:1910.01108v4  [cs.CL]  1 Mar 2020\\nIn this paper, we show that it is possible to reach similar performances on many downstream-tasks\\nusing much smaller language models pre-trained with knowledge distillation, resulting in models\\nthat are lighter and faster at inference time, while also requiring a smaller computational training\\nbudget. Our general-purpose pre-trained models can be ﬁne-tuned with good performances on several\\ndownstream tasks, keeping the ﬂexibility of larger models. We also show that our compressed models\\nare small enough to run on the edge, e.g. on mobile devices.\\nUsing a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained\\nthrough distillation via the supervision of a bigger Transformer language model can achieve similar\\nperformance on a variety of downstream tasks, while being 60% faster at inference time. Further',\n",
       "  'through distillation via the supervision of a bigger Transformer language model can achieve similar\\nperformance on a variety of downstream tasks, while being 60% faster at inference time. Further\\nablation studies indicate that all the components of the triple loss are important for best performances.\\nWe have made the trained weights available along with the training code in the Transformers2\\nlibrary from HuggingFace [Wolf et al., 2019].\\n2 Knowledge distillation\\nKnowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which\\na compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher or an ensemble of models.\\nIn supervised learning, a classiﬁcation model is generally trained to predict an instance class by\\nmaximizing the estimated probability of gold labels. A standard training objective thus involves\\nminimizing the cross-entropy between the model’s predicted distribution and the one-hot empirical\\ndistribution of training labels. A model performing well on the training set will predict an output\\ndistribution with high probability on the correct class and with near-zero probabilities on other\\nclasses. But some of these \"near-zero\" probabilities are larger than others and reﬂect, in part, the\\ngeneralization capabilities of the model and how well it will perform on the test set3.',\n",
       "  'generalization capabilities of the model and how well it will perform on the test set3.\\nTraining loss The student is trained with a distillation loss over the soft target probabilities of\\nthe teacher: Lce=P\\niti\\x03log(si)where ti(resp. si) is a probability estimated by the teacher\\n(resp. the student). This objective results in a rich training signal by leveraging the full teacher\\ndistribution. Following Hinton et al. [2015] we used a softmax-temperature :pi=exp(zi=T)P\\njexp(zj=T)\\nwhere Tcontrols the smoothness of the output distribution and ziis the model score for the class i.\\nThe same temperature Tis applied to the student and the teacher at training time, while at inference,\\nTis set to 1 to recover a standard softmax .\\nThe ﬁnal training objective is a linear combination of the distillation loss Lcewith the supervised\\ntraining loss, in our case the masked language modeling lossLmlm [Devlin et al., 2018]. We found it\\nbeneﬁcial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student\\nand teacher hidden states vectors.\\n3 DistilBERT: a distilled version of BERT',\n",
       "  'and teacher hidden states vectors.\\n3 DistilBERT: a distilled version of BERT\\nStudent architecture In the present work, the student - DistilBERT - has the same general architecture as BERT. The token-type embeddings and the pooler are removed while the number of layers\\nis reduced by a factor of 2. Most of the operations used in the Transformer architecture ( linear\\nlayer andlayer normalisation ) are highly optimized in modern linear algebra frameworks and our\\ninvestigations showed that variations on the last dimension of the tensor (hidden size dimension) have\\na smaller impact on computation efﬁciency (for a ﬁxed parameters budget) than variations on other\\nfactors like the number of layers. Thus we focus on reducing the number of layers.\\nStudent initialization In addition to the previously described optimization and architectural choices,\\nan important element in our training procedure is to ﬁnd the right initialization for the sub-network to\\nconverge. Taking advantage of the common dimensionality between teacher and student networks,\\nwe initialize the student from the teacher by taking one layer out of two.\\n2https://github.com/huggingface/transformers\\n3E.g. BERT-base’s predictions for a masked token in \" I think this is the beginning of a',\n",
       "  '3E.g. BERT-base’s predictions for a masked token in \" I think this is the beginning of a\\nbeautiful [MASK] \" comprise two high probability tokens ( dayandlife) and a long tail of valid predictions\\n(future ,story ,world . . . ).\\n2\\nTable 1: DistilBERT retains 97% of BERT performance. Comparison on the dev sets of the\\nGLUE benchmark. ELMo results as reported by the authors. BERT and DistilBERT results are the\\nmedians of 5 runs with different seeds.\\nModel Score CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B WNLI\\nELMo 68.7 44.1 68.6 76.6 71.1 86.2 53.4 91.5 70.4 56.3\\nBERT-base 79.5 56.3 86.7 88.6 91.8 89.6 69.3 92.7 89.0 53.5\\nDistilBERT 77.0 51.3 82.2 87.5 89.2 88.5 59.9 91.3 86.9 56.3\\nTable 2: DistilBERT yields to comparable\\nperformance on downstream tasks. Comparison on downstream tasks: IMDb (test accuracy) and SQuAD 1.1 (EM/F1 on dev set).',\n",
       "  'performance on downstream tasks. Comparison on downstream tasks: IMDb (test accuracy) and SQuAD 1.1 (EM/F1 on dev set).\\nD: with a second step of distillation during\\nﬁne-tuning.\\nModel IMDb SQuAD\\n(acc.) (EM/F1)\\nBERT-base 93.46 81.2/88.5\\nDistilBERT 92.82 77.7/85.8\\nDistilBERT (D) - 79.1/86.9Table 3: DistilBERT is signiﬁcantly smaller\\nwhile being constantly faster. Inference\\ntime of a full pass of GLUE task STS-B (sentiment analysis) on CPU with a batch size of\\n1.\\nModel # param. Inf. time\\n(Millions) (seconds)\\nELMo 180 895\\nBERT-base 110 668\\nDistilBERT 66 410\\nDistillation We applied best practices for training BERT model recently proposed in Liu et al. [2019].\\nAs such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K\\nexamples per batch) using dynamic masking and without the next sentence prediction objective.\\nData and compute power We train DistilBERT on the same corpus as the original BERT model:',\n",
       "  'examples per batch) using dynamic masking and without the next sentence prediction objective.\\nData and compute power We train DistilBERT on the same corpus as the original BERT model:\\na concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT\\nwas trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the\\nRoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100.\\n4 Experiments\\nGeneral Language Understanding We assess the language understanding and generalization capabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark\\n[Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems.\\nWe report scores on the development sets for each task by ﬁne-tuning DistilBERT without the use\\nof ensembling or multi-tasking scheme for ﬁne-tuning (which are mostly orthogonal to the present\\nwork). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters\\net al. [2018]) encoder followed by two BiLSTMs.4\\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of',\n",
       "  'et al. [2018]) encoder followed by two BiLSTMs.4\\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of\\nindividual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo\\nbaseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to\\nBERT, retaining 97% of the performance with 40% fewer parameters.\\n4.1 Downstream task benchmark\\nDownstream tasks We further study the performances of DistilBERT on several downstream tasks\\nunder efﬁcient inference constraints: a classiﬁcation task (IMDb sentiment classiﬁcation - Maas et al.\\n[2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]).\\nAs shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb\\nbenchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT.\\nWe also studied whether we could add another step of distillation during the adaptation phase by',\n",
       "  'We also studied whether we could add another step of distillation during the adaptation phase by\\nﬁne-tuning DistilBERT on SQuAD using a BERT model previously ﬁne-tuned on SQuAD as a\\n4We use jiant [Wang et al., 2019] to compute the baseline.\\n3\\nTable 4: Ablation study. Variations are relative to the model trained with triple loss and teacher\\nweights initialization.\\nAblation Variation on GLUE macro-score\\n;-Lcos-Lmlm -2.96\\nLce-;-Lmlm -1.46\\nLce-Lcos-; -0.31\\nTriple loss + random weights initialization -3.69\\nteacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two\\nsuccessive steps of distillation, one during the pre-training phase and one during the adaptation phase.\\nIn this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and\\n70.4 EM, i.e. within 3 points of the full model.\\nSize and inference speed\\nTo further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number',\n",
       "  'Size and inference speed\\nTo further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number\\nof parameters of each model along with the inference time needed to do a full pass on the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1.\\nDistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT.\\nOn device computation We studied whether DistilBERT could be used for on-the-edge applications\\nby building a mobile application for question answering. We compare the average inference time on\\na recent smartphone (iPhone 7 Plus) against our previously trained question answering model based\\non BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole\\nmodel weighs 207 MB (which could be further reduced with quantization). Our code is available5.\\n4.2 Ablation study\\nIn this section, we investigate the inﬂuence of various components of the triple loss and the student\\ninitialization on the performances of the distilled model. We report the macro-score on GLUE. Table 4\\npresents the deltas with the full triple loss: removing the Masked Language Modeling loss has little',\n",
       "  'presents the deltas with the full triple loss: removing the Masked Language Modeling loss has little\\nimpact while the two distillation losses account for a large portion of the performance.\\n5 Related work\\nTask-speciﬁc distillation Most of the prior works focus on building task-speciﬁc distillation setups. Tang et al. [2019] transfer ﬁne-tune classiﬁcation model BERT to an LSTM-based classiﬁer.\\nChatterjee [2019] distill BERT model ﬁne-tuned on SQuAD in a smaller Transformer model previously initialized from BERT. In the present work, we found it beneﬁcial to use a general-purpose\\npre-training distillation rather than a task-speciﬁc distillation. Turc et al. [2019] use the original\\npretraining objective to train smaller student, then ﬁne-tuned via distillation. As shown in the ablation study, we found it beneﬁcial to leverage the teacher’s knowledge to pre-train with additional\\ndistillation signal.\\nMulti-distillation Yang et al. [2019] combine the knowledge of an ensemble of teachers using\\nmulti-task learning to regularize the distillation. The authors apply Multi-Task Knowledge Distillation',\n",
       "  'multi-task learning to regularize the distillation. The authors apply Multi-Task Knowledge Distillation\\nto learn a compact question answering model from a set of large question answering models. An\\napplication of multi-distillation is multi-linguality: Tsai et al. [2019] adopts a similar approach to us\\nby pre-training a multilingual model from scratch solely through distillation. However, as shown in\\nthe ablation study, leveraging the teacher’s knowledge with initialization and additional losses leads\\nto substantial gains.\\nOther compression techniques have been studied to compress large models. Recent developments\\nin weights pruning reveal that it is possible to remove some heads in the self-attention at test time\\nwithout signiﬁcantly degrading the performance Michel et al. [2019]. Some layers can be reduced\\nto one head. A separate line of study leverages quantization to derive smaller models (Gupta et al.\\n[2015]). Pruning and quantization are orthogonal to the present work.\\n5https://github.com/huggingface/swift-coreml-transformers\\n4\\n6 Conclusion and future work\\nWe introduced DistilBERT, a general-purpose pre-trained version of BERT, 40% smaller, 60% faster,\\nthat retains 97% of the language understanding capabilities. We showed that a general-purpose',\n",
       "  'that retains 97% of the language understanding capabilities. We showed that a general-purpose\\nlanguage model can be successfully trained with distillation and analyzed the various components\\nwith an ablation study. We further demonstrated that DistilBERT is a compelling option for edge\\napplications.\\nReferences\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. In NAACL-HLT , 2018.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\\nunsupervised multitask learners. 2019.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke S. Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv ,\\nabs/1907.11692, 2019.\\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. ArXiv , abs/1907.10597, 2019.\\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in',\n",
       "  'Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in\\nnlp. In ACL, 2019.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need. In NIPS , 2017.\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\\nTim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Transformers: State-of-the-art natural language\\nprocessing, 2019.\\nCristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD , 2006.\\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv ,\\nabs/1503.02531, 2015.\\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja',\n",
       "  'Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja\\nFidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\\nbooks. 2015 IEEE International Conference on Computer Vision (ICCV) , pages 19–27, 2015.\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A\\nmulti-task benchmark and analysis platform for natural language understanding. In ICLR , 2018.\\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\\nZettlemoyer. Deep contextualized word representations. In NAACL , 2018.\\nAlex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari, Shuning\\nJin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave, Najoung Kim, Phu Mon\\nHtut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu, Anhad Mohananey, Shikha Bordia, Nicolas',\n",
       "  'Htut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu, Anhad Mohananey, Shikha Bordia, Nicolas\\nPatry, Ellie Pavlick, and Samuel R. Bowman. jiant 1.1: A software toolkit for research on general-purpose\\ntext understanding models. http://jiant.info/ , 2019.\\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng, and Christopher Potts. Learning\\nword vectors for sentiment analysis. In ACL, 2011.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine\\ncomprehension of text. In EMNLP , 2016.\\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-speciﬁc\\nknowledge from bert into simple neural networks. ArXiv , abs/1903.12136, 2019.\\nDebajyoti Chatterjee. Making neural machine reading comprehension faster. ArXiv , abs/1904.00796, 2019.\\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact',\n",
       "  'Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact\\nof student initialization on knowledge distillation. ArXiv , abs/1908.08962, 2019.\\nZe Yang, Linjun Shou, Ming Gong, Wutao Lin, and Daxin Jiang. Model compression with multi-task knowledge\\ndistillation for web-scale question answering system. ArXiv , abs/1904.09636, 2019.\\nHenry Tsai, Jason Riesa, Melvin Johnson, Naveen Arivazhagan, Xin Li, and Amelia Archer. Small and practical\\nbert models for sequence labeling. In EMNLP-IJCNLP , 2019.\\nPaul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In NeurIPS , 2019.\\nSuyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited\\nnumerical precision. In ICML , 2015.\\n5',\n",
       "  'Constructing Datasets\\nfor Multi-hop Reading Comprehension Across Documents\\nJohannes Welbl1Pontus Stenetorp1Sebastian Riedel1;2\\n1University College London,2Bloomsbury AI\\nfj.welbl,p.stenetorp,s.riedel g@cs.ucl.ac.uk\\nAbstract\\nMost Reading Comprehension methods limit\\nthemselves to queries which can be answered\\nusing a single sentence, paragraph, or document. Enabling models to combine disjoint\\npieces of textual evidence would extend the\\nscope of machine comprehension methods,\\nbut currently no resources exist to train and\\ntest this capability. We propose a novel task to\\nencourage the development of models for text\\nunderstanding across multiple documents and\\nto investigate the limits of existing methods.\\nIn our task, a model learns to seek and combine evidence – effectively performing multihop, alias multi-step, inference. We devise a\\nmethodology to produce datasets for this task,\\ngiven a collection of query-answer pairs and\\nthematically linked documents. Two datasets\\nfrom different domains are induced,1and we\\nidentify potential pitfalls and devise circumvention strategies. We evaluate two previously proposed competitive models and ﬁnd',\n",
       "  'thematically linked documents. Two datasets\\nfrom different domains are induced,1and we\\nidentify potential pitfalls and devise circumvention strategies. We evaluate two previously proposed competitive models and ﬁnd\\nthat one can integrate information across documents. However, both models struggle to select relevant information; and providing documents guaranteed to be relevant greatly improves their performance. While the models outperform several strong baselines, their\\nbest accuracy reaches 54.5% on an annotated\\ntest set, compared to human performance at\\n85.0%, leaving ample room for improvement.\\n1 Introduction\\nDevising computer systems capable of answering\\nquestions about knowledge described using text has\\n1Available at http://qangaroo.cs.ucl.ac.uk\\nThe Hanging Gardens, in [Mumbai], also known as Pherozeshah Mehta Gardens, are terraced gardens … They provide sunset views over the [Arabian Sea] …\\nMumbai (also known as Bombay, the ofﬁcial name until 1995) is the capital city of the Indian state of Maharashtra. It is the most populous city in India …Q: (Hanging gardens of Mumbai, country, ?)  Options:  {Iran, India, Pakistan, Somalia, …}',\n",
       "  'The Arabian Sea is a region of the northern Indian Ocean bounded on the north by Pakistan and Iran, on the west by northeastern Somalia and the Arabian Peninsula, and on the east by India …Figure 1: A sample from the W IKIHOPdataset where it\\nis necessary to combine information spread across multiple documents to infer the correct answer.\\nbeen a longstanding challenge in Natural Language\\nProcessing (NLP). Contemporary end-to-end Reading Comprehension (RC) methods can learn to extract the correct answer span within a given text\\nand approach human-level performance (Kadlec et\\nal., 2016; Seo et al., 2017a). However, for existing datasets, relevant information is often concentrated locally within a single sentence, emphasizing\\nthe role of locating, matching, and aligning information between query and support text. For example,\\nWeissenborn et al. (2017) observed that a simple binary word-in-query indicator feature boosted the relative accuracy of a baseline model by 27.9%.\\nWe argue that, in order to further the ability of machine comprehension methods to extract knowledge\\nfrom text, we must move beyond a scenario where\\nrelevant information is coherently and explicitly\\nstated within a single document. Methods with this',\n",
       "  'from text, we must move beyond a scenario where\\nrelevant information is coherently and explicitly\\nstated within a single document. Methods with this\\ncapability would aid Information Extraction (IE) applications, such as discovering drug-drug interac-arXiv:1710.06481v2  [cs.CL]  11 Jun 2018\\ntions (Gurulingappa et al., 2012) by connecting protein interactions reported across different publications. They would also beneﬁt search (Carpineto and\\nRomano, 2012) and Question Answering (QA) applications (Lin and Pantel, 2001) where the required\\ninformation cannot be found in a single location.\\nFigure 1 shows an example from W IKIPEDIA ,\\nwhere the goal is to identify the country property\\nof the Hanging Gardens of Mumbai . This cannot be\\ninferred solely from the article about them without\\nadditional background knowledge, as the answer is\\nnot stated explicitly. However, several of the linked\\narticles mention the correct answer India (and other\\ncountries), but cover different topics (e.g. Mumbai ,\\nArabian Sea , etc.). Finding the answer requires\\nmulti-hop reasoning: ﬁguring out that the Hanging\\nGardens are located in Mumbai , and then, from a\\nsecond document, that Mumbai is a city in India .',\n",
       "  'Gardens are located in Mumbai , and then, from a\\nsecond document, that Mumbai is a city in India .\\nWe deﬁne a novel RC task in which a model\\nshould learn to answer queries by combining evidence stated across documents. We introduce a\\nmethodology to induce datasets for this task and derive two datasets. The ﬁrst, W IKIHOP, uses sets of\\nWIKIPEDIA articles where answers to queries about\\nspeciﬁc properties of an entity cannot be located in\\nthe entity’s article. In the second dataset, M EDHOP,\\nthe goal is to establish drug-drug interactions based\\non scientiﬁc ﬁndings about drugs and proteins and\\ntheir interactions, found across multiple M EDLINE\\nabstracts. For both datasets we draw upon existing\\nKnowledge Bases (KBs), W IKIDATA and D RUGBANK, as ground truth, utilizing distant supervision (Mintz et al., 2009) to induce the data – similar\\nto Hewlett et al. (2016) and Joshi et al. (2017).\\nWe establish that for 74.1% and 68.0% of the\\nsamples, the answer can be inferred from the given\\ndocuments by a human annotator. Still, constructing multi-document datasets is challenging; we encounter and prescribe remedies for several pitfalls',\n",
       "  'samples, the answer can be inferred from the given\\ndocuments by a human annotator. Still, constructing multi-document datasets is challenging; we encounter and prescribe remedies for several pitfalls\\nassociated with their assembly – for example, spurious co-locations of answers and speciﬁc documents.\\nFor both datasets we then establish several strong\\nbaselines and evaluate the performance of two previously proposed competitive RC models (Seo et al.,\\n2017a; Weissenborn et al., 2017). We ﬁnd that one\\ncan integrate information across documents, but neither excels at selecting relevant information from a\\nlarger documents set, as their accuracy increases sig-niﬁcantly when given only documents guaranteed to\\nbe relevant. The best model reaches 54.5% on an\\nannotated test set, compared to human performance\\nat 85.0%, indicating ample room for improvement.\\nIn summary, our key contributions are as follows:\\nFirstly, proposing a cross-document multi-step RC\\ntask, as well as a general dataset induction strategy. Secondly, assembling two datasets from different domains and identifying dataset construction\\npitfalls and remedies. Thirdly, establishing multiple\\nbaselines, including two recently proposed RC models, as well as analysing model behaviour in detail\\nthrough ablation studies.\\n2 Task and Dataset Construction Method\\nWe will now formally deﬁne the multi-hop RC task,',\n",
       "  'through ablation studies.\\n2 Task and Dataset Construction Method\\nWe will now formally deﬁne the multi-hop RC task,\\nand a generic methodology to construct multi-hop\\nRC datasets. Later, in Sections 3 and 4 we will\\ndemonstrate how this method is applied in practice\\nby creating datasets for two different domains.\\nTask Formalization A model is given a query q, a\\nset of supporting documents Sq, and a set of candidate answers Cq– all of which are mentioned in Sq.\\nThe goal is to identify the correct answer a\\x032Cq\\nby drawing on the support documents Sq. Queries\\ncould potentially have several true answers when not\\nconstrained to rely on a speciﬁc set of support documents – e.g., queries about the parent of a certain\\nindividual. However, in our setup each sample has\\nonly one true answer among CqandSq. Note that\\neven though we will utilize background information\\nduring dataset assembly, such information will not\\nbe available to a model: the document set will be\\nprovided in random order and without any metadata.\\nWhile certainly beneﬁcial, this would distract from\\nour goal of fostering end-to-end RC methods that infer facts by combining separate facts stated in text.\\nDataset Assembly We assume that there exists a\\ndocument corpus D, together with a KB containing',\n",
       "  'Dataset Assembly We assume that there exists a\\ndocument corpus D, together with a KB containing\\nfact triples (s; r; o )– with subject entity s, relation r,\\nand object entity o. For example, one such fact\\ncould be (Hanging Gardens ofMumbai, country,\\nIndia) . We start with individual KB facts and transform them into query-answer pairs by leaving the\\nobject slot empty, i.e. q= (s; r;?)anda\\x03=o.\\nNext, we deﬁne a directed bipartite graph, where\\nvertices on one side correspond to documents in\\nD, and vertices on the other side are entities from\\nthe KB – see Figure 2 for an example. A document node dis connected to an entity eifeis mentioned in d, though there may be further constraints\\nwhen deﬁning the graph connectivity. For a given\\n(q; a\\x03)pair, the candidates Cqand support documents Sq\\x12Dare identiﬁed by traversing the bipartite graph using breadth-ﬁrst search; the documents\\nvisited will become the support documents Sq.\\nAs the traversal starting point, we use the node\\nbelonging to the subject entity sof the query q. As\\ntraversal end points, we use the set of all entity nodes',\n",
       "  'belonging to the subject entity sof the query q. As\\ntraversal end points, we use the set of all entity nodes\\nthat are type-consistent answers to q.2Note that\\nwhenever there is another fact (s; r; o0)in the KB,\\ni.e. a fact producing the same qbut with a different\\na\\x03, we will not include o0into the set of end points\\nfor this sample. This ensures that precisely one of\\nthe end points corresponds to a correct answer to q.3\\nWhen traversing the graph starting at s, several\\nend points will be visited, though generally not all;\\nthose visited deﬁne the candidate set Cq. If however\\nthe correct answer a\\x03is not among them we discard\\nthe(q; a\\x03)pair. The documents visited to reach the\\nend points will deﬁne the support document set Sq.\\nThat is, Sqcomprises chains of documents leading\\nnot only from the query subject to the correct answer, but also to type-consistent false candidates.\\nWith this methodology, relevant textual evidence\\nfor(q; a\\x03)will be spread across documents along\\nthe chain connecting sanda\\x03– ensuring that multihop reasoning goes beyond resolving co-reference\\nwithin a single document. Note that including\\nother type-consistent candidates alongside a\\x03as end',\n",
       "  'the chain connecting sanda\\x03– ensuring that multihop reasoning goes beyond resolving co-reference\\nwithin a single document. Note that including\\nother type-consistent candidates alongside a\\x03as end\\npoints in the graph traversal – and thus into the support documents – renders the task considerably more\\nchallenging (Jia and Liang, 2017). Models could\\notherwise identify a\\x03in the documents by simply\\nrelying on type-consistency heuristics. It is worth\\npointing out that by introducing alternative candidates we counterbalance a type-consistency bias, in\\ncontrast to Hermann et al. (2015) and Hill et al.\\n(2016) who instead rely on entity masking.\\n2To determine entities which are type-consistent for a\\nquery q, we consider all entities which are observed as object\\nin a fact with ras relation type – including the correct answer.\\n3Here we rely on a closed-world assumption; that is, we assume that the facts in the KB state all true facts.\\nDocuments\\nEntitiesKB\\n(s, r, o)(s, r, o0)(s0,r ,o00)soo0o00\\nFigure 2: A bipartite graph connecting entities and documents mentioning them. Bold edges are those traversed',\n",
       "  'Figure 2: A bipartite graph connecting entities and documents mentioning them. Bold edges are those traversed\\nfor the ﬁrst fact in the small KB on the right; yellow highlighting indicates documents in Sqand candidates in Cq.\\nCheck and cross indicate correct and false candidates.\\n3 W IKIHOP\\nWIKIPEDIA contains an abundance of humancurated, multi-domain information and has several structured resources such as infoboxes and\\nWIKIDATA (Vrande ˇci´c, 2012) associated with it.\\nWIKIPEDIA has thus been used for a wealth of research to build datasets posing queries about a single\\nsentence (Morales et al., 2016; Levy et al., 2017) or\\narticle (Yang et al., 2015; Hewlett et al., 2016; Rajpurkar et al., 2016). However, no attempt has been\\nmade to construct a cross-document multi-step RC\\ndataset based on W IKIPEDIA .\\nA recently proposed RC dataset is W IKIREADING (Hewlett et al., 2016), where W IKIDATA tuples (item, property, answer) are aligned with\\nthe W IKIPEDIA articles regarding their item . The\\ntuples deﬁne a slot ﬁlling task with the goal of predicting the answer , given an article andproperty .',\n",
       "  'tuples deﬁne a slot ﬁlling task with the goal of predicting the answer , given an article andproperty .\\nOne problem with using W IKIREADING as an extractive RC dataset is that 54.4% of the samples\\ndo not state the answer explicitly in the given article (Hewlett et al., 2016). However, we observed\\nthat some of the articles accessible by following hyperlinks from the given article often state the answer,\\nalongside other plausible candidates.\\n3.1 Assembly\\nWe now apply the methodology from Section 2\\nto create a multi-hop dataset with W IKIPEDIA as\\nthe document corpus and W IKIDATA as structured\\nknowledge triples. In this setup, (item, property,\\nanswer) WIKIDATA tuples correspond to (s; r; o )\\ntriples, and the item andproperty of each sample\\ntogether form our query q– e.g., (Hanging Gardens\\nof Mumbai, country, ?) . Similar to Yang et al. (2015)\\nwe only use the ﬁrst paragraph of an article, as relevant information is more often stated in the beginning. Starting with all samples in W IKIREADING ,\\nwe ﬁrst remove samples where the answer is stated\\nexplicitly in the W IKIPEDIA article about the item .4',\n",
       "  'we ﬁrst remove samples where the answer is stated\\nexplicitly in the W IKIPEDIA article about the item .4\\nThe bipartite graph is structured as follows:\\n(1) for edges from articles to entities: all articles\\nmentioning an entity eare connected to e; (2) for\\nedges from entities to articles: each entity eis only\\nconnected to the W IKIPEDIA article about the entity.\\nTraversing the graph is then equivalent to iteratively\\nfollowing hyperlinks to new articles about the anchor text entities.\\nFor a given query-answer pair, the item entity\\nis chosen as the starting point for the graph traversal. A traversal will always pass through the article\\nabout the item , since this is the only document connected from there. The end point set includes the\\ncorrect answer alongside other type-consistent candidate expressions, which are determined by considering allfacts belonging to W IKIREADING training samples, selecting those triples with the same\\nproperty as inqand keeping their answer expressions. As an example, for the W IKIDATA property\\ncountry , this would be the set fFrance ;Russia ; :::g.\\nWe executed graph traversal up to a maximum chain\\nlength of 3 documents. To not pose unreasonable\\ncomputational constraints, samples with more than\\n64 different support documents or 100 candidates',\n",
       "  'We executed graph traversal up to a maximum chain\\nlength of 3 documents. To not pose unreasonable\\ncomputational constraints, samples with more than\\n64 different support documents or 100 candidates\\nare removed, discarding \\x191% of the samples.\\n3.2 Mitigating Dataset Biases\\nDataset creation is always fraught with the risk of\\ninducing unintended errors and biases (Chen et al.,\\n2016; Schwartz et al., 2017). As Hewlett et al.\\n(2016) only carried out limited analysis of their\\nWIKIREADING dataset, we present an analysis of\\nthe downstream effects we observe on W IKIHOP.\\nCandidate Frequency Imbalance A ﬁrst observation is that there is a signiﬁcant bias in the answer\\ndistribution of W IKIREADING . For example, in the\\nmajority of the samples the property country has\\ntheUnited States of America as the answer. A simple\\n4We thus use a disjoint subset of W IKIREADING compared\\nto Levy et al. (2017) to construct W IKIHOP.majority class baseline would thus prove successful,\\nbut would tell us little about multi-hop reasoning. To\\ncombat this issue, we subsampled the dataset to ensure that samples of any one particular answer candidate make up no more than 0:1%of the dataset,',\n",
       "  'combat this issue, we subsampled the dataset to ensure that samples of any one particular answer candidate make up no more than 0:1%of the dataset,\\nand omitted articles about the United States .\\nDocument-Answer Correlations A problem\\nunique to our multi-document setting is the possibility of spurious correlations between candidates and\\ndocuments induced by the graph traversal method.\\nIn fact, if we were notto address this issue, a model\\ndesigned to exploit these regularities could achieve\\n74.6% accuracy (detailed in Section 6).\\nConcretely, we observed that certain documents\\nfrequently co-occur with the correct answer, independently of the query. For example, if the article\\nabout London is present in Sq, the answer is likely\\nto be the United Kingdom , independent of the query\\ntype or entity in question. Appendix C contains a\\nlist with several additional examples.\\nWe designed a statistic to measure this effect\\nand then used it to sub-sample the dataset. The\\nstatistic counts how often a candidate cis observed\\nas the correct answer when a certain document is\\npresent in Sqacross training set samples. More formally, for a given document dand answer candidatec, letcooccurrence (d; c)denote the total count\\nof how often dco-occurs with cin a sample where\\ncis also the correct answer. We use this statistic',\n",
       "  'of how often dco-occurs with cin a sample where\\ncis also the correct answer. We use this statistic\\nto ﬁlter the dataset, by discarding samples with at\\nleast one document-candidate pair (d; c)for which\\ncooccurrence (d; c)>20.\\n4 M EDHOP\\nFollowing the same general methodology, we next\\nconstruct a second dataset for the domain of molecular biology – a ﬁeld that has been undergoing exponential growth in the number of publications (Cohen and Hunter, 2004). The promise of applying\\nNLP methods to cope with this increase has led to\\nresearch efforts in IE (Hirschman et al., 2005; Kim\\net al., 2011) and QA for biomedical text (Hersh et\\nal., 2007; Nentidis et al., 2017). There are a plethora\\nof manually curated structured resources (Ashburner\\net al., 2000; The UniProt Consortium, 2017) which\\ncan either serve as ground truth or to induce training\\ndata using distant supervision (Craven and Kumlien,\\n1999; Bobic et al., 2012). Existing RC datasets are\\neither severely limited in size (Hersh et al., 2007)\\nor cover a very diverse set of query types (Nentidis',\n",
       "  'either severely limited in size (Hersh et al., 2007)\\nor cover a very diverse set of query types (Nentidis\\net al., 2017), complicating the application of neural models that have seen successes for other domains (Wiese et al., 2017).\\nA task that has received signiﬁcant attention is\\ndetecting Drug-Drug Interactions (DDIs). Existing DDI efforts have focused on explicit mentions\\nof interactions in single sentences (Gurulingappa\\net al., 2012; Percha et al., 2012; Segura-Bedmar\\net al., 2013). However, as shown by Peng et al.\\n(2017), cross-sentence relation extraction increases\\nthe number of available relations. It is thus likely\\nthat cross-document interactions would further improve recall, which is of particular importance considering interactions that are never stated explicitly\\n– but rather need to be inferred from separate pieces\\nof evidence. The promise of multi-hop methods is\\nﬁnding and combining individual observations that\\ncan suggest previously unobserved DDIs, aiding the\\nprocess of making scientiﬁc discoveries, yet not directly from experiments, but by inferring them from\\nestablished public knowledge (Swanson, 1986).\\nDDIs are caused by Protein-Protein Interaction (PPI) chains, forming biomedical pathways.\\nIf we consider PPI chains across documents,',\n",
       "  'DDIs are caused by Protein-Protein Interaction (PPI) chains, forming biomedical pathways.\\nIf we consider PPI chains across documents,\\nwe ﬁnd examples like in Figure 3. Here the\\nﬁrst document states that the drug Leuprolide\\ncauses GnRH receptor -induced synaptic potentiations, which can be blocked by the protein\\nProgonadoliberin-1 . The last document states that\\nanother drug, Triptorelin , is a superagonist of the\\nsame protein. It is therefore likely to affect the potency of Leuprolide , describing a way in which the\\ntwo drugs interact. Besides the true interaction there\\nis also a false candidate Urofollitropin for which,\\nalthough mentioned together with GnRH receptor\\nwithin one document, there is no textual evidence\\nindicating interactions with Leuprolide .\\n4.1 Assembly\\nWe construct M EDHOPusing D RUGBANK (Law\\net al., 2014) as structured knowledge resource and\\nresearch paper abstracts from M EDLINE as documents. There is only one relation type for D RUGBANK facts, interacts with , that connects pairs of\\ndrugs – an example of a M EDHOPquery would thus\\nQ:  (Leuprolide, interacts_with, ?)  Options:  {Triptorelin, Urofollitropin}',\n",
       "  'Q:  (Leuprolide, interacts_with, ?)  Options:  {Triptorelin, Urofollitropin}\\nLeuprolide ... elicited a long-lasting potentiation of excitatory postsynaptic currents… [GnRH receptor]-induced synaptic potentiation was blocked … by [Progonadoliberin-1], a speciﬁc [GnRH receptor] antagonist…\\nAnalyses of gene expression demonstrated a dynamic response to the Progonadoliberin-1 superagonist Triptorelin.\\n… our research to study the distribution, co-localization of Urofollitropin and its receptor[,] and co-localization of Urofollitropin and GnRH receptor…Figure 3: A sample from the M EDHOPdataset.\\nbe(Leuprolide, interacts with, ?) . We start\\nby processing the 2016 M EDLINE release using the\\npreprocessing pipeline employed for the BioNLP\\n2011 Shared Task (Stenetorp et al., 2011). We restrict the set of entities in the bipartite graph to\\ndrugs in D RUGBANK and human proteins in S WISS PROT (Bairoch et al., 2004). That is, the graph has\\ndrugs and proteins on one side, and M EDLINE abstracts on the other.',\n",
       "  'drugs and proteins on one side, and M EDLINE abstracts on the other.\\nThe edge structure is as follows: (1) There is an\\nedge from a document to all proteins mentioned in it.\\n(2) There is an edge between a document and a drug,\\nif this document also mentions a protein known to be\\na target for the drug according to D RUGBANK. This\\nedge is bidirectional, i.e. it can be traversed both\\nways, since there is no canonical document describing each drug – thus one can “hop” to any document\\nmentioning the drug and its target. (3) There is an\\nedge from a protein pto a document mentioning p,\\nbut only if the document also mentions another proteinp0which is known to interact with paccording to\\nREACTOME (Fabregat et al., 2016). Given our distant supervision assumption, these additionally constraining requirements err on the side of precision.\\nAs a mention, similar to Percha et al. (2012), we\\nconsider any exact match of a name variant of a\\ndrug or human protein in D RUGBANK or S WISS PROT. For a given DDI (drug 1, interacts with,\\ndrug 2), we then select drug 1as the starting point\\nfor the graph traversal. As possible end points, we\\nconsider any other drug, apart from drug 1and those',\n",
       "  'drug 2), we then select drug 1as the starting point\\nfor the graph traversal. As possible end points, we\\nconsider any other drug, apart from drug 1and those\\ninteracting with drug 1other than drug 2. Similar to\\nWIKIHOP, we exclude samples with more than 64\\nsupport documents and impose a maximum document length of 300 tokens plus title.\\nDocument Sub-sampling The bipartite graph for\\nMEDHOPis orders of magnitude more densely connected than for W IKIHOP. This can lead to potentially large support document sets Sq, to a degree\\nwhere it becomes computationally infeasible for a\\nmajority of existing RC models. After the traversal has ﬁnished, we subsample documents by ﬁrst\\nadding a set of documents that connects the drug in\\nthe query with its answer. We then iteratively add\\ndocuments to connect alternative candidates until we\\nreach the limit of 64 documents – while ensuring\\nthat all candidates have the same number of paths\\nthrough the bipartite graph.\\nMitigating Candidate Frequency Imbalance\\nSome drugs interact with more drugs than others\\n–Aspirin for example interacts with 743 other\\ndrugs, but Isotretinoin with only 34. This leads\\nto similar candidate frequency imbalance issues\\nas with W IKIHOP– but due to its smaller size',\n",
       "  'drugs, but Isotretinoin with only 34. This leads\\nto similar candidate frequency imbalance issues\\nas with W IKIHOP– but due to its smaller size\\nMEDHOPis difﬁcult to sub-sample. Nevertheless\\nwe can successfully combat this issue by masking\\nentity names, detailed in Section 6.2.\\n5 Dataset Analysis\\nTable 1 shows the dataset sizes. Note that W IKIHOPinherits the train, development, and test set\\nsplits from W IKIREADING – i.e., the full dataset\\ncreation, ﬁltering, and sub-sampling pipeline is executed on each set individually. Also note that subsampling according to document-answer correlation\\nsigniﬁcantly reduces the size of W IKIHOPfrom\\n\\x19528K training samples to \\x1944K. While in terms of\\nsamples, both W IKIHOPand M EDHOPare smaller\\nthan other large-scale RC datasets, such as SQuAD\\nand W IKIREADING , the supervised learning signal\\navailable per sample is arguably greater. One could,\\nfor example, re-frame the task as binary path classiﬁcation: given two entities and a document path',\n",
       "  'available per sample is arguably greater. One could,\\nfor example, re-frame the task as binary path classiﬁcation: given two entities and a document path\\nconnecting them, determine whether a given relation holds. For such a case, W IKIHOPand M EDHOPwould have more than 1M and 150K paths to\\nbe classiﬁed, respectively. Instead, in our formulation, this corresponds to each single sample containing the supervised learning signal from an average\\nof 19.5 and 59.8 unique document paths.\\nTable 2 shows statistics on the number of candidates and documents per sample on the respective\\ntraining sets. For M EDHOP, the majority of samples have 9 candidates, due to the way documentsTrain Dev Test Total\\nWIKIHOP 43,738 5,129 2,451 51,318\\nMEDHOP 1,620 342 546 2,508\\nTable 1: Dataset sizes for our respective datasets.\\nmin max avg median\\n# cand. – WH 2 79 19.8 14\\n# docs. – WH 3 63 13.7 11\\n# tok/doc – WH 4 2,046 100.4 91\\n# cand. – MH 2 9 8.9 9\\n# docs. – MH 5 64 36.4 29\\n# tok/doc – MH 5 458 253.9 264',\n",
       "  '# cand. – MH 2 9 8.9 9\\n# docs. – MH 5 64 36.4 29\\n# tok/doc – MH 5 458 253.9 264\\nTable 2: Candidates and documents per sample and document length statistics. WH: W IKIHOP; MH: M EDHOP.\\nare selected up until a maximum of 64 documents is\\nreached. Few samples have less than 9 candidates,\\nand samples would have far more false candidates if\\nmore than 64 support documents were included. The\\nnumber of query types in W IKIHOPis 277, whereas\\nin M EDHOPthere is only one: interacts with .\\n5.1 Qualitative Analysis\\nTo establish the quality of the data and analyze potential distant supervision errors, we sampled and\\nannotated 100 samples from each development set.\\nWIKIHOP Table 3 lists characteristics along with\\nthe proportion of samples that exhibit them. For\\n45%, the true answer either uniquely follows from\\nmultiple texts directly or is suggested as likely. For\\n26%, more than one candidate is plausibly supported by the documents, including the correct answer. This is often due to hypernymy, where\\nthe appropriate level of granularity for the answer is difﬁcult to predict – e.g. (west suffolk,\\nadministrative entity, ?) with candidates',\n",
       "  'the appropriate level of granularity for the answer is difﬁcult to predict – e.g. (west suffolk,\\nadministrative entity, ?) with candidates\\nsuffolk and england . This is a direct consequence of including type-consistent false answer\\ncandidates from W IKIDATA , which can lead to questions with several true answers. For 9% of the\\ncases a single document sufﬁces; these samples\\ncontain a document that states enough information\\nabout item and answer together. For example,\\nthe query (Louis Auguste, father, ?) has the\\ncorrect answer Louis XIV of France , and French\\nUnique multi-step answer. 36%\\nLikely multi-step unique answer. 9%\\nMultiple plausible answers. 15%\\nAmbiguity due to hypernymy. 11%\\nOnly single document required. 9%\\nAnswer does not follow. 12%\\nWIKIDATA /WIKIPEDIA discrepancy. 8%\\nTable 3: Qualitiative analysis of W IKIHOPsamples.\\nking Louis XIV is mentioned within the same document as Louis Auguste . Finally, although our\\ntask is signiﬁcantly more complex than most previous tasks where distant supervision has been applied, the distant supervision assumption is only violated for 20% of the samples – a proportion similar to previous work (Riedel et al., 2010). These',\n",
       "  'cases can either be due to conﬂicting information between W IKIDATA and W IKIPEDIA (8%), e.g. when\\nthe date of birth for a person differs between W IKIDATA and what is stated in the W IKIPEDIA article,\\nor because the answer is consistent but cannot be\\ninferred from the support documents (12%). When\\nanswering 100 questions, the annotator knew the answer prior to reading the documents for 9%, and produced the correct answer after reading the document\\nsets for 74% of the cases. On 100 questions of a validated portion of the Dev set (see Section 5.3), 85%\\naccuracy was reached.\\nMEDHOP Since both document complexity and\\nnumber of documents per sample were signiﬁcantly\\nlarger compared to W IKIHOP, (see Figure 4 in Appendix B) it was not feasible to ask an annotator to read allsupport documents for 100 samples.\\nWe opted to verify the dataset quality by providing\\nonly the subset of documents relevant to support the\\ncorrect answer, i.e., those traversed along the path\\nreaching the answer. The annotator was asked if the\\nanswer to the query “follows” ,“is likely” , or“does\\nnot follow” , given the relevant documents. 68% of',\n",
       "  'answer to the query “follows” ,“is likely” , or“does\\nnot follow” , given the relevant documents. 68% of\\nthe cases were considered as “follows” or as “is\\nlikely” . The majority of cases violating the distant\\nsupervision assumption were due to lacking a necessary PPI in one of the connecting documents.5.2 Crowdsourced Human Annotation\\nWe asked human annotators on Amazon Mechanical\\nTurk to evaluate samples of the W IKIHOPdevelopment set. Similar to our qualitative analysis of M EDHOP, annotators were shown the query-answer pair\\nas a fact and the chain of relevant documents leading\\nto the answer. They were then instructed to answer\\n(1) whether they knew the fact before; (2) whether\\nthe fact follows from the texts (with options “fact\\nfollows” ,“fact is likely” , and “fact does not follow” ); and (3); whether a single or several of the\\ndocuments are required. Each sample was shown to\\nthree annotators and a majority vote was used to aggregate the annotations. Annotators were familiar\\nwith the fact 4.6% of the time; prior knowledge of\\nthe fact is thus not likely to be a confounding effect\\non the other judgments. Inter-annotator agreement',\n",
       "  'with the fact 4.6% of the time; prior knowledge of\\nthe fact is thus not likely to be a confounding effect\\non the other judgments. Inter-annotator agreement\\nas measured by Fleiss’ kappa is 0.253 in (2), and\\n0.281 in (3) – indicating a fair overall agreement, according to Landis and Koch (1977). Overall, 9.5%\\nof samples have no clear majority in (2).\\nAmong samples with a majority judgment, 59.8%\\nare cases where the fact “follows” , for 14.2% the\\nfact is judged as “likely” , and as “not follow” for\\n25.9%. This again provides good justiﬁcation for\\nthe distant supervision strategy.\\nAmong the samples with a majority vote for (2)\\nof either “follows” or“likely” , 55.9% were marked\\nwith a majority vote as requiring multiple documents to infer the fact, and 44.1% as requiring only\\na single document. The latter number is larger than\\ninitially expected, given the construction of samples\\nthrough graph traversal. However, when inspecting\\ncases judged as “single” more closely, we observed',\n",
       "  'initially expected, given the construction of samples\\nthrough graph traversal. However, when inspecting\\ncases judged as “single” more closely, we observed\\nthat many indeed provide a clear hint about the correct answer within one document, but without stating it explicitly. For example, for the fact (witold\\ncichy, country ofcitizenship, poland) with\\ndocuments d1: Witold Cichy (born March 15, 1986\\nin Wodzisaw lski) is a Polish footballer[...] andd2:\\nWodzisaw lski[...] is a town in Silesian Voivodeship,\\nsouthern Poland[...] , the information provided in d1\\nsufﬁces for a human given the background knowledge that Polish is an attribute related to Poland , removing the need for d2to infer the answer.\\n5.3 Validated Test Sets\\nWhile training models on distantly supervised data\\nis useful, one should ideally evaluate methods on a\\nmanually validated test set. We thus identiﬁed subsets of the respective test sets for which the correct\\nanswer can be inferred from the text. This is in contrast to prior work such as Hermann et al. (2015),\\nHill et al. (2016), and Hewlett et al. (2016), who\\nevaluate only on distantly supervised samples. For',\n",
       "  'Hill et al. (2016), and Hewlett et al. (2016), who\\nevaluate only on distantly supervised samples. For\\nWIKIHOP, we applied the same annotation strategy\\nas described in Section 5.2. The validated test set\\nconsists of those samples labeled by a majority of\\nannotators (at least 2 of 3) as “follows” , and requiring“multiple” documents. While desirable, crowdsourcing is not feasible for M EDHOPsince it requires specialist knowledge. In addition, the number\\nof document paths is \\x193x larger, which along with\\nthe complexity of the documents greatly increases\\nthe annotation time. We thus manually annotated\\n20% of the M EDHOPtest set and identiﬁed the samples for which the text implies the correct answer\\nand where multiple documents are required.\\n6 Experiments\\nThis section describes experiments on W IKIHOP\\nand M EDHOPwith the goal of establishing the performance of several baseline models, including recent neural RC models. We empirically demonstrate\\nthe importance of mitigating dataset biases, probe\\nwhether multi-step behavior is beneﬁcial for solving the task, and investigate if RC models can learn\\nto perform lexical abstraction. Training will be conducted on the respective training sets, and evaluation',\n",
       "  'to perform lexical abstraction. Training will be conducted on the respective training sets, and evaluation\\non both the full test set and validated portion (Section 5.3) allowing for a comparison between the two.\\n6.1 Models\\nRandom Selects a random candidate; note that the\\nnumber of candidates differs between samples.\\nMax-mention Predicts the most frequently mentioned candidate in the support documents Sqof a\\nsample – randomly breaking ties.\\nMajority-candidate-per-query-type Predicts the\\ncandidate c2Cqthat was most frequently observed\\nas the true answer in the training set, given the querytype of q. For W IKIHOP, the query type is the propertypof the query; for M EDHOPthere is only the\\nsingle query type – interacts with .\\nTF-IDF Retrieval-based models are known to be\\nstrong QA baselines if candidate answers are provided (Clark et al., 2016; Welbl et al., 2017). They\\nsearch for individual documents based on keywords\\nin the question, but typically do not combine information across documents. The purpose of this baseline is to see if it is possible to identify the correct\\nanswer from a single document alone through lexical correlations. The model forms its prediction as\\nfollows: For each candidate c, the concatenation of\\nthe query qwithcis fed as an ORquery into the',\n",
       "  'follows: For each candidate c, the concatenation of\\nthe query qwithcis fed as an ORquery into the\\nwhoosh text retrieval engine.5It then predicts the\\ncandidate with the highest TF-IDF similarity score:\\narg max\\nc2Cq[max\\ns2Sq(TF-IDF (q+c; s))] (1)\\nDocument-cue During dataset construction we\\nobserved that certain document-answer pairs appear\\nmore frequently than others, to the effect that the\\ncorrect candidate is often indicated solely by the\\npresence of certain documents in Sq. This baseline\\ncaptures how easy it is for a model to exploit these\\ninformative document-answer co-occurrences. It\\npredicts the candidate with highest score across Cq:\\narg max\\nc2Cq[max\\nd2Sq(cooccurrence (d; c))] (2)\\nExtractive RC models: FastQA and BiDAF In\\nour experiments we evaluate two recently proposed\\nLSTM -based extractive QA models: the Bidirectional Attention Flow model ( BiDAF , Seo et al.\\n(2017a)), and FastQA (Weissenborn et al., 2017),\\nwhich have shown a robust performance across several datasets. These models predict an answer span',\n",
       "  '(2017a)), and FastQA (Weissenborn et al., 2017),\\nwhich have shown a robust performance across several datasets. These models predict an answer span\\nwithin a single document. We adapt them to a multidocument setting by sequentially concatenating all\\nd2Sqin random order into a superdocument,\\nadding document separator tokens. During training,\\nthe ﬁrst answer mention in the concatenated document serves as the gold span.6At test time, we measured accuracy based on the exact match between\\n5https://pypi.python.org/pypi/Whoosh/\\n6We also tested assigning the gold span randomly to any\\none of the mention of the answer, with insigniﬁcant changes.\\nthe prediction and answer, both lowercased, after removing articles, trailing white spaces and punctuation, in the same way as Rajpurkar et al. (2016).\\nTo rule out any signal stemming from the order of\\ndocuments in the superdocument, this order is randomized both at training and test time. In a preliminary experiment we also trained models using different random document order permutations, but found\\nthat performance did not change signiﬁcantly.\\nForBiDAF , the default hyperparameters from the\\nimplementation of Seo et al. (2017a) are used, with',\n",
       "  'ForBiDAF , the default hyperparameters from the\\nimplementation of Seo et al. (2017a) are used, with\\npretrained GloVe (Pennington et al., 2014) embeddings. However, we restrict the maximum document length to 8,192 tokens and hidden size to 20,\\nand train for 5,000 iterations with batchsize 16 in order to ﬁt the model into memory.7ForFastQA we\\nuse the implementation provided by the authors, also\\nwith pre-trained GloVe embeddings, no characterembeddings, no maximum support length, hidden\\nsize 50, and batch size 64 for 50 epochs.\\nWhile BiDAF andFastQA were initially developed and tested on single-hop RC datasets, their usage of bidirectional LSTMs and attention over the\\nfull sequence theoretically gives them the capacity\\nto integrate information from different locations in\\nthe (super-)document. In addition, BiDAF employs\\niterative conditioning across multiple layers, potentially making it even better suited to integrate information found across the sequence.\\n6.2 Lexical Abstraction: Candidate Masking\\nThe presence of lexical regularities among answers is a problem in RC dataset assembly – a\\nphenomenon already observed by Hermann et al.\\n(2015). When comprehending a text, the correct answer should become clear from its context – rather',\n",
       "  'phenomenon already observed by Hermann et al.\\n(2015). When comprehending a text, the correct answer should become clear from its context – rather\\nthan from an intrinsic property of the answer expression. To evaluate the ability of models to rely\\non context alone, we created masked versions of\\nthe datasets: we replace any candidate expression\\nrandomly using 100 unique placeholder tokens, e.g.\\n“Mumbai is the most populous city in MASK7 . ”\\nMasking is consistent within one sample, but generally different for the same expression across samples. This not only removes answer frequency cues,\\n7The superdocument has a larger number of tokens compared to e.g. SQuAD , thus the additional memory requirements.Model Unﬁltered Filtered\\nDocument-cue 74.6 36.7\\nMaj. candidate 41.2 38.8\\nTF-IDF 43.8 25.6\\nTrain set size 527,773 43,738\\nTable 4: Accuracy comparison for simple baseline models on W IKIHOPbefore andafter ﬁltering.\\nit also removes statistical correlations between frequent answer strings and support documents. Models consequently cannot base their prediction on intrinsic properties of the answer expression, but have\\nto rely on the context surrounding the mentions.\\n6.3 Results and Discussion',\n",
       "  'to rely on the context surrounding the mentions.\\n6.3 Results and Discussion\\nTable 5 shows the experimental outcomes for W IKIHOPand M EDHOP, together with results for the\\nmasked setting; we will ﬁrst discuss the former. A\\nﬁrst observation is that candidate mention frequency\\ndoes not produce better predictions than a random\\nguess. Predicting the answer most frequently observed at training time achieves strong results: as\\nmuch as 38.8% / 44.2% and 58.4% / 67.3% on the\\ntwo datasets, for the full and validated test sets respectively. That is, a simple frequency statistic together with answer type constraints alone is a relatively strong predictor, and the strongest overall for\\nthe“unmasked” version of M EDHOP.\\nThe TF-IDF retrieval baseline clearly performs\\nbetter than random for W IKIHOP, but is not very\\nstrong overall. That is, the question tokens are helpful to detect relevant documents, but exploiting only\\nthis information compares poorly to the other baselines. On the other hand, as no co-mention of an\\ninteracting drug pair occurs within any single document in M EDHOP, the TF-IDF baseline performs\\nworse than random. We conclude that lexical matching with a single support document is not enough to\\nbuild a strong predictive model for both datasets.',\n",
       "  'worse than random. We conclude that lexical matching with a single support document is not enough to\\nbuild a strong predictive model for both datasets.\\nTheDocument-cue baseline can predict more than\\na third of the samples correctly, for both datasets,\\neven after sub-sampling frequent document-answer\\npairs for W IKIHOP. The relative strength of this\\nand other baselines proves to be an important issue when designing multi-hop datasets, which we\\naddressed through the measures described in SecWIKIHOP MEDHOP\\nstandard masked standard masked\\nModel test test* test test* test test* test test*\\nRandom 11.5 12.2 12.2 13.0 13.9 20.4 14.1 22.4\\nMax-mention 10.6 15.9 13.9 20.1 9.5 16.3 9.2 16.3\\nMajority-candidate-per-query-type 38.8 44.2 12.0 13.7 58.4 67.3 10.4 6.1\\nTF-IDF 25.6 36.7 14.4 24.2 9.0 14.3 8.8 14.3\\nDocument-cue 36.7 41.7 7.4 20.3 44.9 53.1 15.2 16.3',\n",
       "  'Document-cue 36.7 41.7 7.4 20.3 44.9 53.1 15.2 16.3\\nFastQA 25.7 27.2 35.8 38.0 23.1 24.5 31.3 30.6\\nBiDAF 42.9 49.7 54.5 59.8 47.8 61.2 33.7 42.9\\nTable 5: Test accuracies for the W IKIHOPand M EDHOPdatasets, both in standard (unmasked) and masked setup.\\nColumns marked with asterisk are for the validated portion of the dataset.\\nWIKIHOP MEDHOP\\nstandard gold chain standard gold chain\\nModel test test* test test* test test* test test*\\nBiDAF 42.9 49.7 57.9 63.4 47.8 61.2 86.4 89.8\\nBiDAF mask 54.5 59.8 81.2 85.7 33.7 42.9 99.3 100.0\\nFastQA 25.7 27.2 44.5 53.5 23.1 24.5 54.6 59.2\\nFastQA mask 35.8 38.0 65.3 70.0 31.3 30.6 51.8 55.1\\nTable 6: Test accuracy comparison when only using documents leading to the correct answer (gold chain). Columns',\n",
       "  'Table 6: Test accuracy comparison when only using documents leading to the correct answer (gold chain). Columns\\nwith asterisk hold results for the validated samples.\\ntion 3.2. In Table 4 we compare the two relevant\\nbaselines on W IKIHOPbefore and after applying\\nﬁltering measures. The absolute strength of these\\nbaselines before ﬁltering shows how vital addressing this issue is: 74.6% accuracy could be reached\\nthrough exploiting the cooccurrence (d; c)statistic\\nalone. This underlines the paramount importance of\\ninvestigating and addressing dataset biases that otherwise would confound seemingly strong RC model\\nperformance. The relative drop demonstrates that\\nthe measures undertaken successfully mitigate the\\nissue. A downside to aggressive ﬁltering is a significantly reduced dataset size, rendering it infeasible\\nfor smaller datasets like M EDHOP.\\nAmong the two neural models, BiDAF is overall\\nstrongest across both datasets – this is in contrast to\\nthe reported results for SQuAD where their performance is nearly indistinguishable. This is possibly\\ndue to the iterative latent interactions in the BiDAF\\narchitecture: we hypothesize that these are of increased importance for our task, where informationis distributed across documents. It is worth emphasizing that unlike the other baselines, both FastQA\\nandBiDAF predict the answer by extracting a span',\n",
       "  'andBiDAF predict the answer by extracting a span\\nfrom the support documents without relying on the\\ncandidate options Cq.\\nIn the masked setup all baseline models reliant on\\nlexical cues fail in the face of the randomized answer\\nexpressions, since the same answer option has different placeholders in different samples. Especially\\non M EDHOP, where dataset sub-sampling is not a\\nviable option, masking proves to be a valuable alternative, effectively circumventing spurious statistical\\ncorrelations that RC models can learn to exploit.\\nBoth neural RC models are able to largely retain\\nor even improve their strong performance when answers are masked: they are able to leverage the textual context of the candidate expressions. To understand differences in model behavior between W IKIHOPand M EDHOP, it is worth noting that drug\\nmentions in M EDHOPare normalized to a unique\\nsingle-word identiﬁer, and performance drops under\\nWIKIHOP MEDHOP\\ntest test* test test*\\nBiDAF 54.5 59.8 33.7 42.9\\nBiDAF rem 44.6 57.7 30.4 36.7\\nFastQA 35.8 38.0 31.3 30.6\\nFastQA rem 38.0 41.2 28.6 24.5\\nTable 7: Test accuracy (masked) when only documents',\n",
       "  'FastQA rem 38.0 41.2 28.6 24.5\\nTable 7: Test accuracy (masked) when only documents\\ncontaining answer candidates are given ( rem).\\nmasking. In contrast, for the open-domain setting of\\nWIKIHOP, a reduction of the answer vocabulary to\\n100 random single-token mask expressions clearly\\nhelps the model in selecting a candidate span, compared to the multi-token candidate expressions in the\\nunmasked setting. Overall, although both neural RC\\nmodels clearly outperform the other baselines, they\\nstill have large room for improvement compared to\\nhuman performance at 74% / 85% for W IKIHOP.\\nComparing results on the full and validated test\\nsets, we observe that the results consistently improve\\non the validated sets. This suggests that the training\\nset contains the signal necessary to make inference\\non valid samples at test time, and that noisy samples\\nare harder to predict.\\n6.4 Using only relevant documents\\nWe conducted further experiments to examine the\\nRC models when presented with only the relevant\\ndocuments in Sq, i.e., the chain of documents leading to the correct answer. This allows us to investigate the hypothetical performance of the models if\\nthey were able to select and read only relevant documents: Table 6 summarizes these results. Models\\nimprove greatly in this gold chain setup, with up',\n",
       "  'they were able to select and read only relevant documents: Table 6 summarizes these results. Models\\nimprove greatly in this gold chain setup, with up\\nto 81.2% / 85.7% on W IKIHOPin the masked setting for BiDAF . This demonstrates that RC models\\nare capable of identifying the answer when few or\\nno plausible false candidates are mentioned, which\\nis particularly evident for M EDHOP, where documents tend to discuss only single drug candidates.\\nIn the masked gold chain setup, models can then\\npick up on what the masking template looks like\\nand achieve almost perfect scores. Conversely, these\\nresults also show that the models’ answer selection process is not robust to the introduction of unrelated documents with type-consistent candidates.This indicates that learning to intelligently select relevant documents before RC may be among the most\\npromising directions for future model development.\\n6.5 Removing relevant documents\\nTo investigate if the neural RC models can draw\\nupon information requiring multi-step inference we\\ndesigned an experiment where we discard all documents that do not contain candidate mentions, including the ﬁrst documents traversed. Table 7 shows\\nthe results: we can observe that performance drops\\nacross the board for BiDAF . There is a signiﬁcant\\ndrop of 3.3%/6.2% on M EDHOP, and 10.0%/2.1%',\n",
       "  'drop of 3.3%/6.2% on M EDHOP, and 10.0%/2.1%\\non W IKIHOP, demonstrating that BiDAF , is able\\nto leverage cross-document information. FastQA\\nshows a slight increase of 2.2%/3.2% for W IKIHOP\\nand a decrease of 2.7%/4.1% on M EDHOP. While\\ninconclusive, it is clear that FastQA with fewer latent interactions than BiDAF has problems integrating cross-document information.\\n7 Related Work\\nRelated Datasets End-to-end text-based QA has\\nwitnessed a surge in interest with the advent of largescale datasets, which have been assembled based\\non F REEBASE (Berant et al., 2013; Bordes et al.,\\n2015), W IKIPEDIA (Yang et al., 2015; Rajpurkar\\net al., 2016; Hewlett et al., 2016), web search\\nqueries (Nguyen et al., 2016), news articles (Hermann et al., 2015; Onishi et al., 2016), books (Hill\\net al., 2016; Paperno et al., 2016), science exams (Welbl et al., 2017), and trivia (Boyd-Graber',\n",
       "  'et al., 2016; Paperno et al., 2016), science exams (Welbl et al., 2017), and trivia (Boyd-Graber\\net al., 2012; Dunn et al., 2017). Besides TriviaQA (Joshi et al., 2017), all these datasets are conﬁned to single documents, and RC typically does not\\nrequire a combination of multiple independent facts.\\nIn contrast, W IKIHOPand M EDHOPare speciﬁcally designed for cross-document RC and multistep inference. There exist other multi-hop RC resources, but they are either very limited in size,\\nsuch as the FraCaS test suite, or based on synthetic\\nlanguage (Weston et al., 2016). TriviaQA partly\\ninvolves multi-step reasoning, but the complexity\\nlargely stems from parsing compositional questions.\\nOur datasets center around compositional inference\\nfrom comparatively simple queries and the crossdocument setup ensures that multi-step inference\\ngoes beyond resolving co-reference.\\nCompositional Knowledge Base Inference\\nCombining multiple facts is common for structured knowledge resources which formulate facts\\nusing ﬁrst-order logic. KB inference methods\\ninclude Inductive Logic Programming (Quinlan,\\n1990; Pazzani et al., 1991; Richards and Mooney,\\n1991) and probabilistic relaxations to logic like',\n",
       "  'include Inductive Logic Programming (Quinlan,\\n1990; Pazzani et al., 1991; Richards and Mooney,\\n1991) and probabilistic relaxations to logic like\\nMarkov Logic (Richardson and Domingos, 2006;\\nSchoenmackers et al., 2008). These approaches\\nsuffer from limited coverage and inefﬁcient inference, though efforts to circumvent sparsity have\\nbeen undertaken (Schoenmackers et al., 2008;\\nSchoenmackers et al., 2010). A more scalable\\napproach to composite rule learning is the Path\\nRanking Algorithm (Lao and Cohen, 2010; Lao et\\nal., 2011), which performs random walks to identify\\nsalient paths between entities. Gardner et al. (2013)\\ncircumvent these sparsity problems by introducing\\nsynthetic links via dense latent embeddings. Several\\nother methods have been proposed, using composition functions such as vector addition (Bordes\\net al., 2014), RNNs (Neelakantan et al., 2015;\\nDas et al., 2017), and memory networks (Jain,\\n2016). Another approach is the Neural Theorem\\nProver (Rockt ¨aschel and Riedel, 2017), which\\nuses dense rule and symbol embeddings to learn a\\ndifferentiable backward chaining algorithm.\\nAll of these previous approaches center around',\n",
       "  'uses dense rule and symbol embeddings to learn a\\ndifferentiable backward chaining algorithm.\\nAll of these previous approaches center around\\nlearning how to combine facts from a KB, i.e., in\\na structured form with pre-deﬁned schema. That\\nis, they work as part of a pipeline, and either rely\\non the output of a previous IE step (Banko et al.,\\n2007), or on direct human annotation (Bollacker et\\nal., 2008) which tends to be costly and biased in coverage. However, recent neural RC methods (Seo et\\nal., 2017a; Shen et al., 2017) have demonstrated that\\nend-to-end language understanding approaches can\\ninfer answers directly from text – sidestepping intermediate query parsing and IE steps. Our work\\naims to evaluate whether end-to-end multi-step RC\\nmodels can indeed operate on raw text documents\\nonly – while performing the kind of inference most\\ncommonly associated with logical inference methods operating on structured knowledge.\\nText-Based Multi-Step Reading Comprehension\\nFried et al. (2015) have demonstrated that exploit-ing information from other related documents based\\non lexical semantic similarity is beneﬁcial for reranking answers in open-domain non-factoid QA.',\n",
       "  'on lexical semantic similarity is beneﬁcial for reranking answers in open-domain non-factoid QA.\\nJansen et al. (2017) chain textual background resources for science exam QA and provide multisentence answer explanations. Beyond, a rich collection of neural models tailored towards multi-step\\nRC has been developed. Memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Kumar\\net al., 2016) deﬁne a model class that iteratively\\nattends over textual memory items, and they show\\npromising performance on synthetic tasks requiring\\nmulti-step reasoning (Weston et al., 2016). One\\ncommon characteristic of neural multi-hop models\\nis their rich structure that enables matching and interaction between question, context, answer candidates and combinations thereof (Peng et al., 2015;\\nWeissenborn, 2016; Xiong et al., 2017; Liu and\\nPerez, 2017), which is often iterated over several\\ntimes (Sordoni et al., 2016; Neumann et al., 2016;\\nSeo et al., 2017b; Hu et al., 2017) and may contain\\ntrainable stopping mechanisms (Graves, 2016; Shen\\net al., 2017). All these methods show promise for\\nsingle-document RC, and by design should be capable of integrating multiple facts across documents.',\n",
       "  'et al., 2017). All these methods show promise for\\nsingle-document RC, and by design should be capable of integrating multiple facts across documents.\\nHowever, thus far they have not been evaluated for a\\ncross-document multi-step RC task – as in this work.\\nLearning Search Expansion Other research addresses expanding the document set available to\\na QA system, either in the form of web navigation (Nogueira and Cho, 2016), or via query\\nreformulation techniques, which often use neural\\nreinforcement learning (Narasimhan et al., 2016;\\nNogueira and Cho, 2017; Buck et al., 2018). While\\nrelated, this work ultimately aims at reformulating\\nqueries to better acquire evidence documents, and\\nnot at answering queries through combining facts.\\n8 Conclusions and Future Work\\nWe have introduced a new cross-document multihop RC task, devised a generic dataset derivation\\nstrategy and applied it to two separate domains. The\\nresulting datasets test RC methods in their ability to\\nperform composite reasoning – something thus far\\nlimited to models operating on structured knowledge\\nresources. In our experiments we found that contemporary RC models can leverage cross-document information, but a sizeable gap to human performance\\nremains. Finally, we identiﬁed the selection of relevant document sets as the most promising direction\\nfor future research.',\n",
       "  'remains. Finally, we identiﬁed the selection of relevant document sets as the most promising direction\\nfor future research.\\nThus far, our datasets center around factoid questions about entities, and as extractive RC datasets,\\nit is assumed that the answer is mentioned verbatim. While this limits the types of questions one can\\nask, these assumptions can facilitate both training\\nand evaluation, and future work – once free-form abstractive answer composition has advanced – should\\nmove beyond. We hope that our work will foster\\nresearch on cross-document information integration,\\nworking towards these long term goals.\\nAcknowledgments\\nWe would like to thank the reviewers and the action editor for their thoughtful and constructive suggestions, as well as Matko Bo ˇsnjak, Tim Dettmers,\\nPasquale Minervini, Jeff Mitchell, and Sebastian\\nRuder for several helpful comments and feedback\\non drafts of this paper. This work was supported by\\nan Allen Distinguished Investigator Award, a Marie\\nCurie Career Integration Award, the EU H2020\\nSUMMA project (grant agreement number 688139),\\nand an Engineering and Physical Sciences Research\\nCouncil scholarship.\\nReferences\\nMichael Ashburner, Catherine A. Ball, Judith A. Blake,\\nDavid Botstein, Heather Butler, J. Michael Cherry, Allan P. Davis, Kara Dolinski, Selina S. Dwight, Janan T.',\n",
       "  'David Botstein, Heather Butler, J. Michael Cherry, Allan P. Davis, Kara Dolinski, Selina S. Dwight, Janan T.\\nEppig, Midori A. Harris, David P. Hill, Laurie IsselTarver, Andrew Kasarskis, Suzanna Lewis, John C.\\nMatese, Joel E. Richardson, Martin Ringwald, Gerald M. Rubin, and Gavin Sherlock. 2000. Gene ontology: tool for the uniﬁcation of biology. Nature Genetics , 25(1):25.\\nAmos Bairoch, Brigitte Boeckmann, Serenella Ferro, and\\nElisabeth Gasteiger. 2004. Swiss-Prot: Juggling between evolution and stability. Brieﬁngs in Bioinformatics , 5(1):39–55.\\nMichele Banko, Michael J. Cafarella, Stephen Soderland,\\nMatt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Proceedings of the\\n20th International Joint Conference on Artiﬁcal Intelligence , IJCAI’07, pages 2670–2676.\\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\\nLiang. 2013. Semantic parsing on freebase fromquestion-answer pairs. In Proceedings of the 2013',\n",
       "  'Jonathan Berant, Andrew Chou, Roy Frostig, and Percy\\nLiang. 2013. Semantic parsing on freebase fromquestion-answer pairs. In Proceedings of the 2013\\nConference on Empirical Methods in Natural Language Processing , pages 1533–1544.\\nTamara Bobic, Roman Klinger, Philippe Thomas, and\\nMartin Hofmann-Apitius. 2012. Improving distantly\\nsupervised extraction of drug-drug and protein-protein\\ninteractions. In Proceedings of the Joint Workshop on\\nUnsupervised and Semi-Supervised Learning in NLP ,\\npages 35–43.\\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\\nSturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human\\nknowledge. In SIGMOD 08 Proceedings of the 2008\\nACM SIGMOD international conference on Management of data , pages 1247–1250.\\nAntoine Bordes, Sumit Chopra, and Jason Weston. 2014.\\nQuestion answering with subgraph embeddings. In\\nEmpirical Methods for Natural Language Processing\\n(EMNLP) , pages 615–620.\\nAntoine Bordes, Nicolas Usunier, Sumit Chopra, and\\nJason Weston. 2015. Large-scale simple question answering with memory networks. CoRR ,\\nabs/1506.02075.',\n",
       "  'Jason Weston. 2015. Large-scale simple question answering with memory networks. CoRR ,\\nabs/1506.02075.\\nJordan Boyd-Graber, Brianna Satinoff, He He, and Hal\\nDaum ´e, III. 2012. Besting the quiz master: Crowdsourcing incremental classiﬁcation games. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and\\nComputational Natural Language Learning , EMNLPCoNLL ’12, pages 1290–1301.\\nChristian Buck, Jannis Bulian, Massimiliano Ciaramita,\\nAndrea Gesmundo, Neil Houlsby, Wojciech Gajewski,\\nand Wei Wang. 2018. Ask the right questions: Active question reformulation with reinforcement learning. International Conference on Learning Representations (ICLR) .\\nClaudio Carpineto and Giovanni Romano. 2012. A survey of automatic query expansion in information retrieval. ACM Comput. Surv. , 44(1):1:1–1:50, January.\\nDanqi Chen, Jason Bolton, and Christopher D. Manning.\\n2016. A thorough examination of the CNN/Daily Mail\\nreading comprehension task. In Proceedings of the\\n54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages\\n2358–2367.',\n",
       "  'reading comprehension task. In Proceedings of the\\n54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages\\n2358–2367.\\nPeter Clark, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Turney, and Daniel\\nKhashabi. 2016. Combining retrieval, statistics, and\\ninference to answer elementary science questions. In\\nProceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence , AAAI’16, pages 2580–2586.\\nKevin Bretonnel Cohen and Lawrence Hunter. 2004.\\nNatural language processing and systems biology. Artiﬁcial Intelligence Methods and Tools for Systems Biology , pages 147–173.\\nMark Craven and Johan Kumlien. 1999. Constructing\\nbiological knowledge bases by extracting information\\nfrom text sources. In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology , pages 77–86.\\nRajarshi Das, Arvind Neelakantan, David Belanger, and\\nAndrew McCallum. 2017. Chains of reasoning over\\nentities, relations, and text using recurrent neural networks. European Chapter of the Association for Computational Linguistics (EACL) , pages 132–141.',\n",
       "  'entities, relations, and text using recurrent neural networks. European Chapter of the Association for Computational Linguistics (EACL) , pages 132–141.\\nMatthew Dunn, Levent Sagun, Mike Higgins, V . Ugur\\nG¨uney, V olkan Cirik, and Kyunghyun Cho. 2017.\\nSearchQA: A new Q&A dataset augmented with context from a search engine. CoRR , abs/1704.05179.\\nAntonio Fabregat, Konstantinos Sidiropoulos, Phani\\nGarapati, Marc Gillespie, Kerstin Hausmann, Robin\\nHaw, Bijay Jassal, Steven Jupe, Florian Korninger,\\nSheldon McKay, Lisa Matthews, Bruce May, Marija Milacic, Karen Rothfels, Veronica Shamovsky,\\nMarissa Webber, Joel Weiser, Mark Williams, Guanming Wu, Lincoln Stein, Henning Hermjakob, and\\nPeter D’Eustachio. 2016. The Reactome pathway knowledgebase. Nucleic Acids Research ,\\n44(D1):D481–D487.\\nDaniel Fried, Peter Jansen, Gustave Hahn-Powell, Mihai\\nSurdeanu, and Peter Clark. 2015. Higher-order lexical semantic models for non-factoid answer reranking.',\n",
       "  'Surdeanu, and Peter Clark. 2015. Higher-order lexical semantic models for non-factoid answer reranking.\\nTransactions of the Association of Computational Linguistics , 3:197–210.\\nMatt Gardner, Partha Pratim Talukdar, Bryan Kisiel, and\\nTom M. Mitchell. 2013. Improving learning and inference in a large knowledge-base using latent syntactic\\ncues. In Proceedings of the Conference on Empirical\\nMethods in Natural Language Processing , pages 833–\\n838.\\nAlex Graves. 2016. Adaptive computation time for recurrent neural networks. CoRR , abs/1603.08983.\\nHarsha Gurulingappa, Abdul Mateen Rajput, Angus\\nRoberts, Juliane Fluck, Martin Hofmann-Apitius, and\\nLuca Toldo. 2012. Development of a benchmark corpus to support the automatic extraction of drug-related\\nadverse effects from medical case reports. Journal of\\nBiomedical Informatics , 45(5):885 – 892. Text Mining and Natural Language Processing in Pharmacogenomics.\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\\nand Phil Blunsom. 2015. Teaching machines to read\\nand comprehend. In Advances in Neural Information',\n",
       "  'and Phil Blunsom. 2015. Teaching machines to read\\nand comprehend. In Advances in Neural Information\\nProcessing Systems , pages 1693–1701.William Hersh, Aaron Cohen, Lynn Ruslen, and Phoebe\\nRoberts. 2007. TREC 2007 genomics track overview.\\nInNIST Special Publication .\\nDaniel Hewlett, Alexandre Lacoste, Llion Jones, Illia\\nPolosukhin, Andrew Fandrianto, Jay Han, Matthew\\nKelcey, and David Berthelot. 2016. WIKIREADING:\\nA novel large-scale language understanding task over\\nWikipedia. In Proceedings of the The 54th Annual\\nMeeting of the Association for Computational Linguistics (ACL 2016) , pages 1535–1545.\\nFelix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. 2016. The goldilocks principle: Reading children’s books with explicit memory representations.\\nICLR .\\nLynette Hirschman, Alexander Yeh, Christian Blaschke,\\nand Alfonso Valencia. 2005. Overview of BioCreAtIvE: Critical assessment of information extraction\\nfor biology. BMC Bioinformatics , 6(1):S1, May.\\nMinghao Hu, Yuxing Peng, and Xipeng Qiu. 2017.\\nMnemonic reader for machine comprehension. CoRR ,',\n",
       "  'Minghao Hu, Yuxing Peng, and Xipeng Qiu. 2017.\\nMnemonic reader for machine comprehension. CoRR ,\\nabs/1705.02798.\\nSarthak Jain. 2016. Question answering over knowledge\\nbase using factual memory networks. In Proceedings\\nof NAACL-HLT , pages 109–115.\\nPeter Jansen, Rebecca Sharp, Mihai Surdeanu, and Peter\\nClark. 2017. Framing QA as building and ranking intersentence answer justiﬁcations. Computational Linguistics , 43(2):407–449.\\nRobin Jia and Percy Liang. 2017. Adversarial examples for evaluating reading comprehension systems. In\\nEmpirical Methods in Natural Language Processing\\n(EMNLP) .\\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale distantly\\nsupervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics , July.\\nRudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan\\nKleindienst. 2016. Text understanding with the attention sum reader network. Proceedings of the 54th\\nAnnual Meeting of the Association for Computational',\n",
       "  'Kleindienst. 2016. Text understanding with the attention sum reader network. Proceedings of the 54th\\nAnnual Meeting of the Association for Computational\\nLinguistics , pages 908–918.\\nJin-Dong Kim, Yue Wang, Toshihisa Takagi, and Akinori\\nYonezawa. 2011. Overview of Genia event task in\\nBioNLP shared task 2011. In Proceedings of BioNLP\\nShared Task 2011 Workshop , pages 7–15.\\nAnkit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer,\\nIshaan Gulrajani James Bradbury, Victor Zhong, Romain Paulus, and Richard Socher. 2016. Ask me\\nanything: Dynamic memory networks for natural language processing. International Conference on Machine Learning , 48:1378–1387.\\nJ. Richard Landis and Gary G. Koch. 1977. The measurement of observer agreement for categorical data.\\nBiometrics , pages 159–174.\\nNi Lao and William W Cohen. 2010. Relational retrieval using a combination of path-constrained random walks. Machine learning , 81(1):53–67.\\nNi Lao, Tom Mitchell, and William W Cohen. 2011.\\nRandom walk inference and learning in a large scale\\nknowledge base. In Proceedings of the Conference on\\nEmpirical Methods in Natural Language Processing ,',\n",
       "  'Random walk inference and learning in a large scale\\nknowledge base. In Proceedings of the Conference on\\nEmpirical Methods in Natural Language Processing ,\\npages 529–539.\\nVivian Law, Craig Knox, Yannick Djoumbou, Tim Jewison, An Chi Guo, Yifeng Liu, Adam Maciejewski, David Arndt, Michael Wilson, Vanessa Neveu,\\nAlexandra Tang, Geraldine Gabriel, Carol Ly, Sakina\\nAdamjee, Zerihun T. Dame, Beomsoo Han, You Zhou,\\nand David S. Wishart. 2014. DrugBank 4.0: Shedding new light on drug metabolism. Nucleic Acids Research , 42(D1):D1091–D1097.\\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-shot relation extraction via reading comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning\\n(CoNLL 2017) , pages 333–342, August.\\nDekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question-answering. Nat. Lang. Eng. ,\\n7(4):343–360, December.\\nFei Liu and Julien Perez. 2017. Gated end-to-end memory networks. In Proceedings of the 15th Conference',\n",
       "  '7(4):343–360, December.\\nFei Liu and Julien Perez. 2017. Gated end-to-end memory networks. In Proceedings of the 15th Conference\\nof the European Chapter of the Association for Computational Linguistics, EACL 2017, Volume 1: Long\\nPapers , pages 1–10.\\nMike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction\\nwithout labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and\\nthe 4th International Joint Conference on Natural Language Processing of the AFNLP , pages 1003–1011.\\nAlvaro Morales, Varot Premtoon, Cordelia Avery, Sue\\nFelshin, and Boris Katz. 2016. Learning to answer\\nquestions from Wikipedia infoboxes. In Proceedings\\nof the 2016 Conference on Empirical Methods in Natural Language Processing , pages 1930–1935.\\nKarthik Narasimhan, Adam Yala, and Regina Barzilay.\\n2016. Improving information extraction by acquiring\\nexternal evidence with reinforcement learning. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016 ,\\npages 2355–2365.\\nArvind Neelakantan, Benjamin Roth, and Andrew McCallum. 2015. Compositional vector space models for',\n",
       "  'pages 2355–2365.\\nArvind Neelakantan, Benjamin Roth, and Andrew McCallum. 2015. Compositional vector space models for\\nknowledge base completion. Proceedings of the 53rd\\nAnnual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Conference\\non Natural Language Processing , pages 156–166.\\nAnastasios Nentidis, Konstantinos Bougiatiotis, Anastasia Krithara, Georgios Paliouras, and Ioannis Kakadiaris. 2017. Results of the ﬁfth edition of the BioASQ\\nchallenge. In BioNLP 2017 , pages 48–57.\\nMark Neumann, Pontus Stenetorp, and Sebastian Riedel.\\n2016. Learning to reason with adaptive computation.\\nInInterpretable Machine Learning for Complex Systems at the 2016 Conference on Neural Information\\nProcessing Systems (NIPS) , Barcelona, Spain, December.\\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\\n2016. MS MARCO: A human generated machine reading comprehension dataset. CoRR ,\\nabs/1611.09268.\\nRodrigo Nogueira and Kyunghyun Cho. 2016. WebNav:',\n",
       "  'abs/1611.09268.\\nRodrigo Nogueira and Kyunghyun Cho. 2016. WebNav:\\nA new large-scale task for natural language based sequential decision making. CoRR , abs/1602.02261.\\nRodrigo Nogueira and Kyunghyun Cho. 2017. Taskoriented query reformulation with reinforcement\\nlearning. Proceedings of the 2017 Conference on\\nEmpirical Methods in Natural Language Processing ,\\npages 574–583.\\nTakeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gimpel, and David A. McAllester. 2016. Who did what:\\nA large-scale person-centered cloze dataset. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016 ,\\npages 2230–2235.\\nDenis Paperno, Germ ´an Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro\\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel\\nFernandez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers) , pages 1525–1534.',\n",
       "  'Papers) , pages 1525–1534.\\nMichael Pazzani, Clifford Brunk, and Glenn Silverstein.\\n1991. A knowledge-intensive approach to learning relational concepts. In Proceedings of the Eighth International Workshop on Machine Learning , pages 432–\\n436, Evanston, IL.\\nBaolin Peng, Zhengdong Lu, Hang Li, and Kam-Fai\\nWong. 2015. Towards neural network-based reasoning. CoRR , abs/1508.05508.\\nNanyun Peng, Hoifung Poon, Chris Quirk, Kristina\\nToutanova, and Wen-tau Yih. 2017. Cross-sentence\\nN-ary relation extraction with graph LSTMs. Transactions of the Association for Computational Linguistics ,\\n5:101–115.\\nJeffrey Pennington, Richard Socher, and Christopher D.\\nManning. 2014. GloVe: Global vectors for word representation. In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP) , pages\\n1532–1543.\\nBethany Percha, Yael Garten, and Russ B Altman. 2012.\\nDiscovery and explanation of drug-drug interactions\\nvia text mining. In Paciﬁc symposium on biocomputing, page 410. NIH Public Access.\\nJohn Ross Quinlan. 1990. Learning logical deﬁnitions',\n",
       "  'John Ross Quinlan. 1990. Learning logical deﬁnitions\\nfrom relations. Machine Learning , 5:239–266.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. SQuAD: 100,000+ questions for\\nmachine comprehension of text. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) , pages 2383–2392.\\nBradley L. Richards and Raymond J. Mooney. 1991.\\nFirst-order theory revision. In Proceedings of the\\nEighth International Workshop on Machine Learning ,\\npages 447–451, Evanston, IL.\\nMatthew Richardson and Pedro Domingos. 2006.\\nMarkov logic networks. Mach. Learn. , 62(1-2):107–\\n136.\\nSebastian Riedel, Limin Yao, and Andrew McCallum.\\n2010. Modeling relations and their mentions without labeled text. In Proceedings of the 2010 European\\nConference on Machine Learning and Knowledge Discovery in Databases: Part III , ECML PKDD’10,\\npages 148–163.\\nTim Rockt ¨aschel and Sebastian Riedel. 2017. End-toend differentiable proving. Advances in Neural Information Processing Systems 30 , pages 3788–3800.',\n",
       "  'Tim Rockt ¨aschel and Sebastian Riedel. 2017. End-toend differentiable proving. Advances in Neural Information Processing Systems 30 , pages 3788–3800.\\nStefan Schoenmackers, Oren Etzioni, and Daniel S.\\nWeld. 2008. Scaling textual inference to the web.\\nInEMNLP ’08: Proceedings of the Conference on\\nEmpirical Methods in Natural Language Processing ,\\npages 79–88.\\nStefan Schoenmackers, Oren Etzioni, Daniel S. Weld,\\nand Jesse Davis. 2010. Learning ﬁrst-order horn\\nclauses from web text. In Proceedings of the 2010\\nConference on Empirical Methods in Natural Language Processing , EMNLP ’10, pages 1088–1098.\\nRoy Schwartz, Maarten Sap, Ioannis Konstas, Leila\\nZilles, Yejin Choi, and Noah A. Smith. 2017. The\\neffect of different writing tasks on linguistic style: A\\ncase study of the ROC story cloze task. In Proceedings of the 21st Conference on Computational Natural\\nLanguage Learning (CoNLL 2017) , pages 15–25.',\n",
       "  'case study of the ROC story cloze task. In Proceedings of the 21st Conference on Computational Natural\\nLanguage Learning (CoNLL 2017) , pages 15–25.\\nIsabel Segura-Bedmar, Paloma Mart ´ınez, and Mar ´ıa Herrero Zazo. 2013. SemEval-2013 Task 9: Extraction of\\ndrug-drug interactions from biomedical texts (DDIExtraction 2013). In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2:Proceedings of the Seventh International Workshop on\\nSemantic Evaluation (SemEval 2013) , pages 341–350.\\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\\nHannaneh Hajishirzi. 2017a. Bidirectional attention\\nﬂow for machine comprehension. In The International\\nConference on Learning Representations (ICLR) .\\nMinjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh\\nHajishirzi. 2017b. Query-reduction networks for\\nquestion answering. ICLR .\\nYelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu\\nChen. 2017. ReasoNet: Learning to stop reading in\\nmachine comprehension. In Proceedings of the 23rd',\n",
       "  'Chen. 2017. ReasoNet: Learning to stop reading in\\nmachine comprehension. In Proceedings of the 23rd\\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’17, pages\\n1047–1055.\\nAlessandro Sordoni, Phillip Bachman, and Yoshua Bengio. 2016. Iterative alternating neural attention for\\nmachine reading. CoRR , abs/1606.02245.\\nPontus Stenetorp, Goran Topi ´c, Sampo Pyysalo, Tomoko\\nOhta, Jin-Dong Kim, and Jun’ichi Tsujii. 2011.\\nBioNLP shared task 2011: Supporting resources. In\\nProceedings of BioNLP Shared Task 2011 Workshop ,\\npages 112–120.\\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and\\nRob Fergus. 2015. End-to-end memory networks. In\\nAdvances in Neural Information Processing Systems ,\\npages 2440–2448.\\nDon R. Swanson. 1986. Undiscovered public knowledge. The Library Quarterly , 56(2):103–118.\\nThe UniProt Consortium. 2017. UniProt: the universal protein knowledgebase. Nucleic Acids Research ,\\n45(D1):D158–D169.',\n",
       "  'The UniProt Consortium. 2017. UniProt: the universal protein knowledgebase. Nucleic Acids Research ,\\n45(D1):D158–D169.\\nDenny Vrande ˇci´c. 2012. Wikidata: A new platform\\nfor collaborative data collection. In Proceedings of\\nthe 21st International Conference on World Wide Web ,\\nWWW ’12 Companion, pages 1063–1064.\\nDirk Weissenborn, Georg Wiese, and Laura Seiffe. 2017.\\nMaking neural QA as simple as possible but not simpler. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017) ,\\npages 271–280. Association for Computational Linguistics.\\nDirk Weissenborn. 2016. Separating answers from\\nqueries for neural reading comprehension. CoRR ,\\nabs/1607.03316.\\nJohannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.\\nCrowdsourcing multiple choice science questions. In\\nProceedings of the Third Workshop on Noisy Usergenerated Text , pages 94–106.\\nJason Weston, Sumit Chopra, and Antoine Bordes. 2015.\\nMemory networks. ICLR .\\nJason Weston, Antoine Bordes, Sumit Chopra, and',\n",
       "  'Jason Weston, Sumit Chopra, and Antoine Bordes. 2015.\\nMemory networks. ICLR .\\nJason Weston, Antoine Bordes, Sumit Chopra, and\\nTomas Mikolov. 2016. Towards AI-complete question answering: A set of prerequisite toy tasks. ICLR .\\nGeorg Wiese, Dirk Weissenborn, and Mariana Neves.\\n2017. Neural question answering at BioASQ 5B. In\\nProceedings of the BioNLP 2017 , pages 76–79.\\nCaiming Xiong, Victor Zhong, and Richard Socher.\\n2017. Dynamic coattention networks for question answering. ICLR .\\nYi Yang, Wen-tau Yih, and Christopher Meek. 2015.\\nWikiQA: A challenge dataset for open-domain question answering. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 2013–2018.\\n0 10 20 30 40 50 60\\nnumber of documents0.000.020.040.060.08dataset proportionWikiHop\\nMedHopFigure 4: Support documents per training sample.\\nA Appendix: Versions\\nThis paper directly corresponds to the TACL version,8apart from minor changes in wording, additional footnotes, and these appendices.\\nB Appendix: Candidate and Document\\nstatistics\\nFigure 4 illustrates the distribution of the number of',\n",
       "  'B Appendix: Candidate and Document\\nstatistics\\nFigure 4 illustrates the distribution of the number of\\nsupport documents per sample. W IKIHOPshows a\\nPoisson-like behaviour – most likely due to structural regularities in W IKIPEDIA – whereas M EDHOP\\nexhibits a bimodal distribution, in line with our observation that certain drugs and proteins have far\\nmore interactions and studies associated with them.\\nFigure 5 shows the distribution of document\\nlengths for both datasets. Note that the document\\nlengths in W IKIHOPcorrespond to the lengths of\\nthe ﬁrst paragraphs of W IKIPEDIA articles. M EDHOPon the other hand reﬂects the length of research\\npaper abstracts, which are generally longer.\\nFigure 6 shows a histogram with the number of\\ncandidates per sample in W IKIHOP, and the distribution shows a slow but steady decrease. For M EDHOP, the vast majority of samples have 9 candidates,\\nwhich is due to the way documents are selected up\\nuntil a maximum of 64 documents is reached. Very\\nfew samples have fewer than 9 candidates, and samples would have far more false candidates if more\\nthan 64 support documents were included.\\n8https://transacl.org/ojs/index.php/\\ntacl/article/view/1325',\n",
       "  'than 64 support documents were included.\\n8https://transacl.org/ojs/index.php/\\ntacl/article/view/1325\\n0 100 200 300 400 500 600\\ndocument length0.000.020.040.060.08dataset proportionWikiHop\\nMedHopFigure 5: Histogram for document lengths in W IKIHOP\\nand M EDHOP.\\n0 10 20 30 40 50 60\\nnumber of candidates0.000.010.020.030.040.05dataset proportionWikiHop\\nFigure 6: Histogram for the number of candidates per\\nsample in W IKIHOP.\\nC Appendix: Document-Cue examples\\nTable 8 shows examples of answers and articles\\nwhich, before ﬁltering, frequently appear together in\\nWIKIHOP.\\nD Appendix: Gold Chain Examples\\nTable 9 shows examples of document gold chains\\nin W IKIHOP. Note that their lengths differ, with a\\nmaximum of 3 documents.\\nE Appendix: Query Types\\nTable 10 gives an overview over the 25 most frequent query types in W IKIHOPand their relative\\nproportion in the dataset. Overall, the distribution\\nacross the 277 query types follows a power law.\\nAnswer a Wikipedia article d Count Prop.\\nunited states of',\n",
       "  'proportion in the dataset. Overall, the distribution\\nacross the 277 query types follows a power law.\\nAnswer a Wikipedia article d Count Prop.\\nunited states of\\namericaAU.S. state is a constituent political entity of the United States of America. 68,233 12.9%\\nunited kingdom England is a country that is part of the United Kingdom. 54,005 10.2%\\ntaxon In biology, a species (abbreviated sp., with the plural form species abbreviated spp.) is the basic unit of biological classiﬁcation and a taxonomic\\nrank.40,141 7.6%\\ntaxon A genus (pl.genera ) is a taxonomic rank used in the biological classiﬁcation38,466 7.3%\\nunited kingdom The United Kingdom of Great Britain and Northern Ireland , commonly\\nknown as the United Kingdom (UK) or Britain, is a sovereign country in\\nwestern Europe.31,071 5.9%\\ntaxon Biology is a natural science concerned with the study of life and living organisms, including their structure, function, growth, evolution, distribution,\\nidentiﬁcation and taxonomy.27,609 5.2%\\nunited kingdom Scotland [...] is a country that is part of the United Kingdom and covers the\\nnorthern third of the island of Great Britain.25,456 4.8%',\n",
       "  'united kingdom Scotland [...] is a country that is part of the United Kingdom and covers the\\nnorthern third of the island of Great Britain.25,456 4.8%\\nunited kingdom Wales [...] is a country that is part of the United Kingdom and the island of\\nGreat Britain.21,961 4.2%\\nunited kingdom London [...] is the capital and most populous city of England and the United\\nKingdom, as well as the most populous city proper in the European Union.21,920 4.2%\\n... ... ...\\nunited states of\\namericaNevada (Spanish for ”snowy”; see pronunciations) is a state in the Western,\\nMountain West, and Southwestern regions of the United States of America.18,215 3.4%\\n... ... ...\\nitaly The comune [...] is a basic administrative division in Italy, roughly equivalent to a township or municipality.8,785 1.7%\\n... ... ...\\nhuman settlementAtown is a human settlement larger than a village but smaller than a city. 5,092 1.0%\\n... ... ...\\npeople’s republic of chinaShanghai [...] often abbreviated as Hu or Shen, is one of the four directcontrolled municipalities of the People’s Republic of China.3,628 0.7%',\n",
       "  'Table 8: Examples with largest cooccurrence (d; c)statistic, before ﬁltering. The Count column states\\ncooccurrence (d; c); the last column states the corresponding relative proportion of training samples (total 527,773).\\nQuery: (the big broadcast of 1937, genre, ?)\\nAnswer: musical ﬁlm\\nText 1: TheBig Broadcast of 1937 is a 1936 Paramount Pictures production directed by Mitchell Leisen,\\nand is the third in the series of Big Broadcast movies. The musical comedy stars Jack Benny, George Burns,\\nGracie Allen, Bob Burns, Martha Raye, Shirley Ross [...]\\nText 2: Shirley Ross (January 7, 1913 March 9, 1975) was an American actress and singer, notable for her\\nduet with Bob Hope, ”Thanks for the Memory” from ”The Big Broadcast of 1938”[...]\\nText 3: The Big Broadcast of 1938 is a Paramount Pictures musical ﬁlm featuring W.C. Fields and Bob\\nHope. Directed by Mitchell Leisen, the ﬁlm is the last in a series of ”Big Broadcast” movies[...]\\nQuery: (cmos, subclass of, ?)\\nAnswer: semiconductor device',\n",
       "  'Query: (cmos, subclass of, ?)\\nAnswer: semiconductor device\\nText 1: Complementary metal-oxide-semiconductor (CMOS) [...] is a technology for constructing integrated circuits. [...] CMOS uses complementary and symmetrical pairs of p-type and n-type metal oxide\\nsemiconductor ﬁeld effect transistors (MOSFETs) for logic functions. [...]\\nText 2: Atransistor is a semiconductor device used to amplify or switch electronic signals[...]\\nQuery: (raik dittrich, sport, ?)\\nAnswer: biathlon\\nText 1: Raik Dittrich (born October 12, 1968 in Sebnitz) is a retired East German biathlete who won two\\nWorld Championships medals. He represented the sports club SG Dynamo Zinnwald [...]\\nText 2: SG Dynamo Zinnwald is a sector of SV Dynamo located in Altenberg, Saxony[...] The main sports\\ncovered by the club are biathlon , bobsleigh, luge, mountain biking, and Skeleton (sport)[...]\\nQuery: (minnesota gubernatorial election, ofﬁce contested, ?)\\nAnswer: governor\\nText 1: The 1936 Minnesota gubernatorial election took place on November 3, 1936. Farmer-Labor Party\\ncandidate Elmer Austin Benson defeated Republican Party of Minnesota challenger Martin A. Nelson.',\n",
       "  'Text 1: The 1936 Minnesota gubernatorial election took place on November 3, 1936. Farmer-Labor Party\\ncandidate Elmer Austin Benson defeated Republican Party of Minnesota challenger Martin A. Nelson.\\nText 2: Elmer Austin Benson [...] served as the 24th governor of Minnesota, defeating Republican Martin\\nNelson in a landslide victory in Minnesota’s 1936 gubernatorial election.[...]\\nQuery: (ieee transactions on information theory, publisher, ?)\\nAnswer: institute of electrical and electronics engineers\\nText 1: IEEE Transactions on Information Theory is a monthly peer-reviewed scientiﬁc journal published by the IEEE Information Theory Society [...] the journal allows the posting of preprints [...]\\nText 2: TheIEEE Information Theory Society (ITS or ITSoc), formerly the IEEE Information Theory\\nGroup, is a professional society of the Institute of Electrical and Electronics Engineers (IEEE) [...]\\nQuery: (country ofcitizenship, louis-philippe ﬁset, ?)\\nAnswer: canada\\nText1: Louis-Philippe Fiset [...] was a local physician and politician in the Mauricie area [...]\\nText2: Mauricie is a traditional and current administrative region of Quebec. La Mauricie National Park is\\ncontained within the region, making it a prime tourist location. [...]\\nText3: La Mauricie National Park is located near Shawinigan in the Laurentian mountains, in the Mauricie',\n",
       "  'contained within the region, making it a prime tourist location. [...]\\nText3: La Mauricie National Park is located near Shawinigan in the Laurentian mountains, in the Mauricie\\nregion of Quebec, Canada [...]\\nTable 9: Examples of document gold chains in W IKIHOP. Article titles are boldfaced, the correct answer is underlined.\\nQuery Type Proportion in Dataset\\ninstance of 10.71 %\\nlocated intheadministrative territorial entity 9.50 %\\noccupation 7.28 %\\nplace ofbirth 5.75 %\\nrecord label 5.27 %\\ngenre 5.03 %\\ncountry ofcitizenship 3.45 %\\nparent taxon 3.16 %\\nplace ofdeath 2.46 %\\ninception 2.20 %\\ndate ofbirth 1.84 %\\ncountry 1.70 %\\nheadquarters location 1.52 %\\npart of 1.43 %\\nsubclass of 1.40 %\\nsport 1.36 %\\nmember ofpolitical party 1.29 %\\npublisher 1.16 %\\npublication date 1.06 %\\ncountry oforigin 0.92 %\\nlanguages spoken orwritten 0.92 %\\ndate ofdeath 0.90 %\\noriginal language ofwork 0.85 %\\nfollowed by 0.82 %\\nposition held 0.79 %\\nTop 25 72.77 %\\nTop 50 86.42 %',\n",
       "  'original language ofwork 0.85 %\\nfollowed by 0.82 %\\nposition held 0.79 %\\nTop 25 72.77 %\\nTop 50 86.42 %\\nTop 100 96.62 %\\nTop 200 99.71 %\\nTable 10: The 25 most frequent query types in W IKIHOPalongside their proportion in the training set.',\n",
       "  'Proximal Policy Optimization Algorithms\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov\\nOpenAI\\n{joschu, filip, prafulla, alec, oleg }@openai.com\\nAbstract\\nWe propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a\\n“surrogate” objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective\\nfunction that enables multiple epochs of minibatch updates. The new methods, which we call\\nproximal policy optimization (PPO), have some of the beneﬁts of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample\\ncomplexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms\\nother online policy gradient methods, and overall strikes a favorable balance between sample\\ncomplexity, simplicity, and wall-time.\\n1 Introduction\\nIn recent years, several diﬀerent approaches have been proposed for reinforcement learning with\\nneural network function approximators. The leading contenders are deep Q-learning [Mni+15],',\n",
       "  'neural network function approximators. The leading contenders are deep Q-learning [Mni+15],\\n“vanilla” policy gradient methods [Mni+16], and trust region / natural policy gradient methods\\n[Sch+15b]. However, there is room for improvement in developing a method that is scalable (to\\nlarge models and parallel implementations), data eﬃcient, and robust (i.e., successful on a variety\\nof problems without hyperparameter tuning). Q-learning (with function approximation) fails on\\nmany simple problems1and is poorly understood, vanilla policy gradient methods have poor data\\neﬃency and robustness; and trust region policy optimization (TRPO) is relatively complicated,\\nand is not compatible with architectures that include noise (such as dropout) or parameter sharing\\n(between the policy and value function, or with auxiliary tasks).\\nThis paper seeks to improve the current state of aﬀairs by introducing an algorithm that attains\\nthe data eﬃciency and reliable performance of TRPO, while using only ﬁrst-order optimization.\\nWe propose a novel objective with clipped probability ratios, which forms a pessimistic estimate\\n(i.e., lower bound) of the performance of the policy. To optimize policies, we alternate between\\nsampling data from the policy and performing several epochs of optimization on the sampled data.',\n",
       "  'sampling data from the policy and performing several epochs of optimization on the sampled data.\\nOur experiments compare the performance of various diﬀerent versions of the surrogate objective, and ﬁnd that the version with the clipped probability ratios performs best. We also compare\\nPPO to several previous algorithms from the literature. On continuous control tasks, it performs\\nbetter than the algorithms we compare against. On Atari, it performs signiﬁcantly better (in terms\\nof sample complexity) than A2C and similarly to ACER though it is much simpler.\\n1While DQN works well on game environments like the Arcade Learning Environment [Bel+15] with discrete\\naction spaces, it has not been demonstrated to perform well on continuous control benchmarks such as those in\\nOpenAI Gym [Bro+16] and described by Duan et al. [Dua+16].\\n1\\narXiv:1707.06347v2  [cs.LG]  28 Aug 2017\\n2 Background: Policy Optimization\\n2.1 Policy Gradient Methods\\nPolicy gradient methods work by computing an estimator of the policy gradient and plugging it\\ninto a stochastic gradient ascent algorithm. The most commonly used gradient estimator has the\\nform\\nˆg=ˆEt/bracketleftBig\\n∇θlogπθ(at|st)ˆAt/bracketrightBig'],\n",
       " 'Similarity Score': [-3.248974,\n",
       "  2.673152,\n",
       "  1.9711704,\n",
       "  1.9334772,\n",
       "  1.0804672,\n",
       "  1.5221784,\n",
       "  1.999823,\n",
       "  1.2737886,\n",
       "  1.7665937,\n",
       "  2.562605,\n",
       "  1.6310401,\n",
       "  1.4679735,\n",
       "  -0.5970006,\n",
       "  -1.6283977,\n",
       "  1.9320099,\n",
       "  2.5884452,\n",
       "  -1.0894024,\n",
       "  -0.4139443,\n",
       "  3.0692568,\n",
       "  0.75248784,\n",
       "  2.3618622,\n",
       "  1.7794173,\n",
       "  2.0369203,\n",
       "  1.927524,\n",
       "  2.2537327,\n",
       "  1.1676681,\n",
       "  1.9371994,\n",
       "  1.1518288,\n",
       "  2.470993,\n",
       "  1.7781737,\n",
       "  2.1508663,\n",
       "  1.6365871,\n",
       "  1.1141975,\n",
       "  1.7803203,\n",
       "  2.6034522,\n",
       "  1.6986425,\n",
       "  1.7013594,\n",
       "  0.064920634,\n",
       "  1.9779079,\n",
       "  2.3764892,\n",
       "  3.0924454,\n",
       "  2.8046901,\n",
       "  2.017971,\n",
       "  0.65531904,\n",
       "  2.9419959,\n",
       "  2.5663195,\n",
       "  1.715646,\n",
       "  1.7005758,\n",
       "  1.2377183,\n",
       "  1.1401525,\n",
       "  1.1284568,\n",
       "  -2.1579483,\n",
       "  0.5193765,\n",
       "  2.15451,\n",
       "  1.5338567,\n",
       "  2.286786,\n",
       "  1.6554543,\n",
       "  2.1509979,\n",
       "  1.1058383,\n",
       "  1.3439826,\n",
       "  3.5011427,\n",
       "  1.9171834,\n",
       "  2.3318088,\n",
       "  3.722729,\n",
       "  2.860323,\n",
       "  1.8135321,\n",
       "  2.3177965,\n",
       "  2.0349383,\n",
       "  1.2184885,\n",
       "  0.89979887,\n",
       "  0.3175267,\n",
       "  0.29018253,\n",
       "  1.3400024,\n",
       "  0.29077855,\n",
       "  0.23768003,\n",
       "  2.3119817,\n",
       "  0.10894878,\n",
       "  2.4711232,\n",
       "  1.2231328,\n",
       "  1.6268666,\n",
       "  1.2825437,\n",
       "  0.570419,\n",
       "  1.262097,\n",
       "  0.9714835,\n",
       "  1.6450374,\n",
       "  -0.24717835,\n",
       "  0.10352447,\n",
       "  -1.149096,\n",
       "  3.3261466,\n",
       "  1.5682063,\n",
       "  -0.73886883,\n",
       "  1.3156714,\n",
       "  2.8465562,\n",
       "  1.6466084,\n",
       "  2.5411892,\n",
       "  -0.4149238,\n",
       "  2.839778,\n",
       "  0.90053505,\n",
       "  1.9926091,\n",
       "  3.0153432]}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Sentence2</th>\n",
       "      <th>Similarity Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td>-3.248974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>loss combining language modeling, distillation...</td>\n",
       "      <td>2.673152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>in real-time has the potential to enable novel...</td>\n",
       "      <td>1.971170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>through distillation via the supervision of a ...</td>\n",
       "      <td>1.933477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>generalization capabilities of the model and h...</td>\n",
       "      <td>1.080467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>contained within the region, making it a prime...</td>\n",
       "      <td>-0.414924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>original language ofwork 0.85 %\\nfollowed by 0...</td>\n",
       "      <td>2.839778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>Proximal Policy Optimization Algorithms\\nJohn ...</td>\n",
       "      <td>0.900535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>neural network function approximators. The lea...</td>\n",
       "      <td>1.992609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>sampling data from the policy and performing s...</td>\n",
       "      <td>3.015343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Query  \\\n",
       "0   What is language model?   \n",
       "1   What is language model?   \n",
       "2   What is language model?   \n",
       "3   What is language model?   \n",
       "4   What is language model?   \n",
       "..                      ...   \n",
       "95  What is language model?   \n",
       "96  What is language model?   \n",
       "97  What is language model?   \n",
       "98  What is language model?   \n",
       "99  What is language model?   \n",
       "\n",
       "                                            Sentence2  Similarity Score  \n",
       "0   DistilBERT, a distilled version of BERT: small...         -3.248974  \n",
       "1   loss combining language modeling, distillation...          2.673152  \n",
       "2   in real-time has the potential to enable novel...          1.971170  \n",
       "3   through distillation via the supervision of a ...          1.933477  \n",
       "4   generalization capabilities of the model and h...          1.080467  \n",
       "..                                                ...               ...  \n",
       "95  contained within the region, making it a prime...         -0.414924  \n",
       "96  original language ofwork 0.85 %\\nfollowed by 0...          2.839778  \n",
       "97  Proximal Policy Optimization Algorithms\\nJohn ...          0.900535  \n",
       "98  neural network function approximators. The lea...          1.992609  \n",
       "99  sampling data from the policy and performing s...          3.015343  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Sentence2</th>\n",
       "      <th>Similarity Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>include Inductive Logic Programming (Quinlan,\\...</td>\n",
       "      <td>3.722729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>they were able to select and read only relevan...</td>\n",
       "      <td>3.501143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>B Appendix: Candidate and Document\\nstatistics...</td>\n",
       "      <td>3.326147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>drugs, but Isotretinoin with only 34. This lea...</td>\n",
       "      <td>3.092445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kr...</td>\n",
       "      <td>3.069257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>Yukun Zhu, Ryan Kiros, Richard S. Zemel, Rusla...</td>\n",
       "      <td>-1.089402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>Jason Weston, Sumit Chopra, and Antoine Bordes...</td>\n",
       "      <td>-1.149096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>multi-task learning to regularize the distilla...</td>\n",
       "      <td>-1.628398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>(2017a)), and FastQA (Weissenborn et al., 2017...</td>\n",
       "      <td>-2.157948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is language model?</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td>-3.248974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Query  \\\n",
       "63  What is language model?   \n",
       "60  What is language model?   \n",
       "88  What is language model?   \n",
       "40  What is language model?   \n",
       "18  What is language model?   \n",
       "..                      ...   \n",
       "16  What is language model?   \n",
       "87  What is language model?   \n",
       "13  What is language model?   \n",
       "51  What is language model?   \n",
       "0   What is language model?   \n",
       "\n",
       "                                            Sentence2  Similarity Score  \n",
       "63  include Inductive Logic Programming (Quinlan,\\...          3.722729  \n",
       "60  they were able to select and read only relevan...          3.501143  \n",
       "88  B Appendix: Candidate and Document\\nstatistics...          3.326147  \n",
       "40  drugs, but Isotretinoin with only 34. This lea...          3.092445  \n",
       "18  Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kr...          3.069257  \n",
       "..                                                ...               ...  \n",
       "16  Yukun Zhu, Ryan Kiros, Richard S. Zemel, Rusla...         -1.089402  \n",
       "87  Jason Weston, Sumit Chopra, and Antoine Bordes...         -1.149096  \n",
       "13  multi-task learning to regularize the distilla...         -1.628398  \n",
       "51  (2017a)), and FastQA (Weissenborn et al., 2017...         -2.157948  \n",
       "0   DistilBERT, a distilled version of BERT: small...         -3.248974  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df.sort_values(by=['Similarity Score'], ascending=False)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_prompt = df_test['Sentence2'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='efficiency on several downstream NLP tasks, including sentiment analysis, text classification, and named entity recognition. We show that DistilBERT achieves performance comparable to its larger counterparts, while requiring less computational resources. Our work provides a more cost-effective solution for deploying large-scale pre-trained models in resource-constrained environments, making state-of-the-art NLP accessible to a wider range of applications.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": rel_prompt.strip()}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cohere in d:\\projects\\new\\env\\lib\\site-packages (4.44)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.0 in d:\\projects\\new\\env\\lib\\site-packages (from cohere) (3.8.6)\n",
      "Requirement already satisfied: backoff<3.0,>=2.0 in d:\\projects\\new\\env\\lib\\site-packages (from cohere) (2.2.1)\n",
      "Requirement already satisfied: fastavro<2.0,>=1.8 in d:\\projects\\new\\env\\lib\\site-packages (from cohere) (1.9.3)\n",
      "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in d:\\projects\\new\\env\\lib\\site-packages (from cohere) (6.8.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.25.0 in d:\\projects\\new\\env\\lib\\site-packages (from cohere) (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in d:\\projects\\new\\env\\lib\\site-packages (from cohere) (1.26.18)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\projects\\new\\env\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in d:\\projects\\new\\env\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\projects\\new\\env\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\projects\\new\\env\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\projects\\new\\env\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\projects\\new\\env\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\projects\\new\\env\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\projects\\new\\env\\lib\\site-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\new\\env\\lib\\site-packages (from requests<3.0.0,>=2.25.0->cohere) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\new\\env\\lib\\site-packages (from requests<3.0.0,>=2.25.0->cohere) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "# init client\n",
    "co = cohere.Client(\"hWm8NH1n6AGGKeou2Awj2RNrn3AEUY1PX73igBlt\")\n",
    "\n",
    "rerank_docs = co.rerank(\n",
    "    query=query, documents=data, top_n=25, model=\"rerank-english-v2.0\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RerankResult<document['text']: that retains 97% of the language understanding capabilities. We showed that a general-purpose\n",
       "language model can be successfully trained with distillation and analyzed the various components\n",
       "with an ablation study. We further demonstrated that DistilBERT is a compelling option for edge\n",
       "applications.\n",
       "References\n",
       "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\n",
       "transformers for language understanding. In NAACL-HLT , 2018.\n",
       "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\n",
       "unsupervised multitask learners. 2019.\n",
       "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\n",
       "Luke S. Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv ,\n",
       "abs/1907.11692, 2019.\n",
       "Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. ArXiv , abs/1907.10597, 2019.\n",
       "Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in, index: 14, relevance_score: 0.9678385>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rerank_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the following question using the provided context.\n",
      "\n",
      "Question: What is language model?\n",
      "\n",
      "Context: \n",
      "RerankResult<document['text']: that retains 97% of the language understanding capabilities. We showed that a general-purpose\n",
      "language model can be successfully trained with distillation and analyzed the various components\n",
      "with an ablation study. We further demonstrated that DistilBERT is a compelling option for edge\n",
      "applications.\n",
      "References\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\n",
      "transformers for language understanding. In NAACL-HLT , 2018.\n",
      "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\n",
      "unsupervised multitask learners. 2019.\n",
      "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\n",
      "Luke S. Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv ,\n",
      "abs/1907.11692, 2019.\n",
      "Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. ArXiv , abs/1907.10597, 2019.\n",
      "Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in, index: 14, relevance_score: 0.9678385>\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rerankprompt = f\"\"\"\n",
    "Answer the following question using the provided context.\n",
    "\n",
    "Question: {query.strip()}\n",
    "\n",
    "Context: \n",
    "{rerank_docs[0]}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Look at the full metaprompt\n",
    "print(rerankprompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='A language model is a type of AI model that is trained to understand and generate human language. It is trained on vast amounts of text data and learns patterns and structures in language in order to generate coherent and contextually appropriate responses. Language models are used in various applications such as chatbots, machine translation, and text generation.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": rerankprompt}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is language model?\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "##End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# creating a DataFrame\n",
    "data = {\n",
    "   'restaurant': ['pizza_hut', 'kfc', 'kitchen'],\n",
    "   'distance': [20, 10, 15],\n",
    "}\n",
    "dataframe = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_restaurants(df, header=True):\n",
    "        return df.to_csv(index=False, header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurant,distance\n",
      "pizza_hut,20\n",
      "kfc,10\n",
      "kitchen,15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "restaurant_list=(format_restaurants(dataframe))\n",
    "print(restaurant_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatGPT(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.url = \"https://api.openai.com/v1/chat/completions\"\n",
    "        self.model = \"gpt-3.5-turbo\"\n",
    "        self.headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer \" + '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',\n",
    "        }\n",
    "        self.prompt = \"Answer the following question, based on the data shown. \" \\\n",
    "            \"Answer in a complete sentence and don't say anything else.\"\n",
    "\n",
    "    def ask(self, restaurants):\n",
    "        content  = self.prompt + \"\\n\\n\" + restaurants \n",
    "        body = {\n",
    "            \"model\":self.model, \n",
    "            \"messages\":[{\"role\": \"user\", \"content\": content}]\n",
    "        }\n",
    "        result = requests.post(\n",
    "            url=self.url,\n",
    "            headers=self.headers,\n",
    "            json=body,\n",
    "        )\n",
    "        return result.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt = ChatGPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The distance to KFC is 10.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatgpt.ask(restaurant_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in d:\\projects\\new\\env\\lib\\site-packages (1.3.7)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in d:\\projects\\new\\env\\lib\\site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\projects\\new\\env\\lib\\site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\projects\\new\\env\\lib\\site-packages (from openai) (0.25.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\projects\\new\\env\\lib\\site-packages (from openai) (2.4.2)\n",
      "Requirement already satisfied: sniffio in d:\\projects\\new\\env\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in d:\\projects\\new\\env\\lib\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in d:\\projects\\new\\env\\lib\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\projects\\new\\env\\lib\\site-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in d:\\projects\\new\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore in d:\\projects\\new\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\projects\\new\\env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in d:\\projects\\new\\env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.10.1)\n",
      "Requirement already satisfied: colorama in d:\\projects\\new\\env\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\projects\\new\\env\\lib\\site-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ask_distance(dataframe):\n",
    "    content = \"measure the least distance with each given restaurant\" +'/n/n' + dataframe\n",
    "    completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    messages=[{\"role\": \"user\", \"content\": content}],\n",
    "    functions=[\n",
    "    {\n",
    "        \"name\": \"get_measure\",\n",
    "        \"description\": \"Get the least distance\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"list of all the restaurants and distances as a dictionary(restuarant_name:distance)\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"distance\"],\n",
    "        },\n",
    "    }\n",
    "        ],\n",
    "        function_call={\"name\":\"get_measure\"}\n",
    "    )\n",
    "    return completion.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_function(query):\n",
    "    # Step 1: send the conversation and available functions to GPT\n",
    "    functions = [\n",
    "    {\n",
    "        \"name\": \"RAG\",\n",
    "        \"description\": \"gets contexts from documents\",\n",
    "        \"parameters\":{\n",
    "            \"type\":\"object\",\n",
    "            \"properties\":{\n",
    "                \"question\":{\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"question describing which context to extract\",\n",
    "                },\n",
    "            },\n",
    "            \"required\":[\"sentences\"]\n",
    "        },\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"ask_distance\",\n",
    "        \"description\": \"Get the temperature at a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"list of all the restaurants and distances as a dictionary(restuarant_name:distance)\",\n",
    "                },\n",
    "\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"distance\"],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        messages=[{\"role\": \"system\", \"content\": query}],\n",
    "        functions=functions,\n",
    "        function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"]\n",
    "    return response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_distance(restaurant_list):\n",
    "    content = \"measure the least distance with each given restaurant\" +'/n/n' + restaurant_list\n",
    "    completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    messages=[{\"role\": \"user\", \"content\": content}],\n",
    "    functions=[\n",
    "    {\n",
    "        \"name\": \"get_measure\",\n",
    "        \"description\": \"Get the least distance\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"list of all the restaurants and distances as a dictionary(restuarant_name:distance)\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"distance\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"RAG\",\n",
    "        \"description\": \"gets contexts from documents\",\n",
    "        \"parameters\":{\n",
    "            \"type\":\"object\",\n",
    "            \"properties\":{\n",
    "                \"question\":{\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"question describing which context to extract\",\n",
    "                },\n",
    "            },\n",
    "            \"required\":[\"sentences\"]\n",
    "        },\n",
    "    },\n",
    "        ],\n",
    "    \n",
    "    )\n",
    "    return completion.choices[0].message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_distance(restaurant_list):\n",
    "    content = \"find the best context from documents\"\n",
    "    completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    messages=[{\"role\": \"user\", \"content\": content}],\n",
    "    functions=[\n",
    "    {\n",
    "        \"name\": \"RAG\",\n",
    "        \"description\": \"get contexts from documents\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"question\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"question describing which context to extract {question}\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"output\"],\n",
    "        },\n",
    "    }\n",
    "        ],\n",
    "        function_call={\"name\":\"RAG\"}\n",
    "    )\n",
    "    return completion.choices[0].message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x1c4a1356570> JSON: {\n",
       "  \"role\": \"assistant\",\n",
       "  \"content\": null,\n",
       "  \"function_call\": {\n",
       "    \"name\": \"RAG\",\n",
       "    \"arguments\": \"{\\n  \\\"question\\\": \\\"What are symptoms of malaria?\\\"\\n}\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_function(\"what are symptopms of malaria?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x24322998290> JSON: {\n",
       "  \"role\": \"assistant\",\n",
       "  \"content\": null,\n",
       "  \"function_call\": {\n",
       "    \"name\": \"get_measure\",\n",
       "    \"arguments\": \"{\\n  \\\"location\\\": \\\"{\\\\\\\"pizza_hut\\\\\\\":20,\\\\\\\"kfc\\\\\\\":10,\\\\\\\"kitchen\\\\\\\":15}\\\"\\n}\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_distance(restaurant_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error response from daemon: Cannot kill container: rag-openai-qdrant: No such container: rag-openai-qdrant\n",
      "Error response from daemon: No such container: rag-openai-qdrant\n"
     ]
    }
   ],
   "source": [
    "!docker kill rag-openai-qdrant\n",
    "!docker rm rag-openai-qdrant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
